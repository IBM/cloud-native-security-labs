{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Native Security \u00b6 What is Cloud Native Security \u00b6 The Cloud Native Computing Foundation (CNCF) publishes the following CNCF Cloud Native Definition v1.0 : \"Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\" We take the liberty to roughly paraphrase that definition as containers, applications and tools running on a flavor of the Kubernetes platform. Cloud Native Security in our view deals with securing applications and tools to secure the full Software Development Life Cycle (SDLC) and Continuous Integration and Continuous Deployment (CI/CD) on a flavor of the Kubernetes platform. For the Cloud Native Security Conference, we have defined 3 tracks: Application Security, Data Security, and DevSecOps. About this workshop \u00b6 The labs in this workshop have originally been created for the Cloud Native Security Conference on June 24-25 and July 1, 2020. Compatability \u00b6 This workshop has been tested on the following platforms: Lab1 : IKS 1.17, 2 worker nodes of flavor b3c.4x16, single zone, and Cloud Shell version 0.6.9 Lab2 : IKS 1.17, 1 worker node of flavor b3c.4x16, single zone, and Cloud Shell version 0.6.9 Lab3 : ROKS 4.3, 3 worker nodes of flavor b3c.4x16, single zone, and Cloud Shell version 0.6.9 Credits \u00b6 Many folks have contributed to help shape, test, and contribute to the workshop. Remko de Knikker Oliver Rodriguez Aya Tokura P. Nigel Brown JJ Asghar Mike Peterson There are many others that deserve credits and we are grateful for everyone's good spirit and support.","title":"About the workshop"},{"location":"#cloud-native-security","text":"","title":"Cloud Native Security"},{"location":"#what-is-cloud-native-security","text":"The Cloud Native Computing Foundation (CNCF) publishes the following CNCF Cloud Native Definition v1.0 : \"Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\" We take the liberty to roughly paraphrase that definition as containers, applications and tools running on a flavor of the Kubernetes platform. Cloud Native Security in our view deals with securing applications and tools to secure the full Software Development Life Cycle (SDLC) and Continuous Integration and Continuous Deployment (CI/CD) on a flavor of the Kubernetes platform. For the Cloud Native Security Conference, we have defined 3 tracks: Application Security, Data Security, and DevSecOps.","title":"What is Cloud Native Security"},{"location":"#about-this-workshop","text":"The labs in this workshop have originally been created for the Cloud Native Security Conference on June 24-25 and July 1, 2020.","title":"About this workshop"},{"location":"#compatability","text":"This workshop has been tested on the following platforms: Lab1 : IKS 1.17, 2 worker nodes of flavor b3c.4x16, single zone, and Cloud Shell version 0.6.9 Lab2 : IKS 1.17, 1 worker node of flavor b3c.4x16, single zone, and Cloud Shell version 0.6.9 Lab3 : ROKS 4.3, 3 worker nodes of flavor b3c.4x16, single zone, and Cloud Shell version 0.6.9","title":"Compatability"},{"location":"#credits","text":"Many folks have contributed to help shape, test, and contribute to the workshop. Remko de Knikker Oliver Rodriguez Aya Tokura P. Nigel Brown JJ Asghar Mike Peterson There are many others that deserve credits and we are grateful for everyone's good spirit and support.","title":"Credits"},{"location":"SUMMARY/","text":"Summary \u00b6 Getting Started \u00b6 Create an IBM Cloud account Setup Workshop \u00b6 Lab0 - Setup Lab1 - Kubernetes Networking Lab2 - Adding Secure Storage Lab3 - Source-to-Image (S2I) Resources \u00b6 Cloud Native Computing Foundation Cloud Native Security Conference IBM Developer Survey \u00b6 Tell us how we did","title":"Summary"},{"location":"SUMMARY/#summary","text":"","title":"Summary"},{"location":"SUMMARY/#getting-started","text":"Create an IBM Cloud account Setup","title":"Getting Started"},{"location":"SUMMARY/#workshop","text":"Lab0 - Setup Lab1 - Kubernetes Networking Lab2 - Adding Secure Storage Lab3 - Source-to-Image (S2I)","title":"Workshop"},{"location":"SUMMARY/#resources","text":"Cloud Native Computing Foundation Cloud Native Security Conference IBM Developer","title":"Resources"},{"location":"SUMMARY/#survey","text":"Tell us how we did","title":"Survey"},{"location":"lab-00/","text":"Setup \u00b6 IBM Kubernetes Service (IKS) and OpenShift \u00b6 For the hands-on labs included in the conference, you can use a free cluster that was created for you to use for the workshop. The cluster will deleted after the conference again. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , Open the URL that was provided to you to access your cluster, Log in to this IBM Cloud account using the workshop code and your IBM Cloud account IBM id, Instructions will ask to Log in to this IBM Cloud account Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, We recommend to use the IBM Cloud Shell at https://shell.cloud.ibm.com/ . It is attached to your IBMid. It might take a few moments to create the instance and a new session, You should now be read to start with Lab 1 or go back to the Summary .","title":"Workshop Setup"},{"location":"lab-00/#setup","text":"","title":"Setup"},{"location":"lab-00/#ibm-kubernetes-service-iks-and-openshift","text":"For the hands-on labs included in the conference, you can use a free cluster that was created for you to use for the workshop. The cluster will deleted after the conference again. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , Open the URL that was provided to you to access your cluster, Log in to this IBM Cloud account using the workshop code and your IBM Cloud account IBM id, Instructions will ask to Log in to this IBM Cloud account Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, We recommend to use the IBM Cloud Shell at https://shell.cloud.ibm.com/ . It is attached to your IBMid. It might take a few moments to create the instance and a new session, You should now be read to start with Lab 1 or go back to the Summary .","title":"IBM Kubernetes Service (IKS) and OpenShift"},{"location":"lab-00/NEWACCOUNT/","text":"IBM Cloud - Create a New Account \u00b6 To create a new account, follow the steps below, You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, go to https://cloud.ibm.com/registration to register, In the Create an account window, enter your email and password, The Verify email section will inform you that a verification code was sent to your email, Switch to your email provider to retrieve the verification code, Enter the verification code in the Verify email section, and click Next , Enter your first name, last name and country in the Personal information section and click Next , Click Create account , Your account is being created, Review the IBM Privacy Statement , Click Proceed to acknowledge the privacy statement, Switch to your email provider to review the Welcome to IBM Cloud email, and click the Login link, Enter your IBM Id to login, Enter your password to login, The IBM Cloud dashboard page should load, You have successfully registered a new IBM Cloud account.","title":"Create an IBM Cloud account"},{"location":"lab-00/NEWACCOUNT/#ibm-cloud-create-a-new-account","text":"To create a new account, follow the steps below, You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, go to https://cloud.ibm.com/registration to register, In the Create an account window, enter your email and password, The Verify email section will inform you that a verification code was sent to your email, Switch to your email provider to retrieve the verification code, Enter the verification code in the Verify email section, and click Next , Enter your first name, last name and country in the Personal information section and click Next , Click Create account , Your account is being created, Review the IBM Privacy Statement , Click Proceed to acknowledge the privacy statement, Switch to your email provider to review the Welcome to IBM Cloud email, and click the Login link, Enter your IBM Id to login, Enter your password to login, The IBM Cloud dashboard page should load, You have successfully registered a new IBM Cloud account.","title":"IBM Cloud - Create a New Account"},{"location":"lab-01/","text":"Lab 01 - Kubernetes Networking, using Service Types, Ingress and Network Policies to Control Application Access \u00b6 Self: https://github.com/remkohdev/kubernetes-networking In this lab we will use several methods to control access to applications on a Kubernetes cluster. We will go through Kubernetes Networking principles and apply different types of Service resources: ClusterIP, NodePort and LoadBalancer. We will also implement an Ingress resource and a Network Policy, while we briefly will talk about Calico policies as well. You need an IBM account to follow the lab. See the Account Setup and Cluster Access In the first steps, you access your cluster, deploy a MongoDB instance and a HelloWorld application, which will take about 15 minutes. During the remainder of the lab, you will create different Service types, an Ingress and a Network Policy. 00. Access Your Cluster \u00b6 Follow the instructions at Account Setup and Cluster Access to claim a pre-created cluster. Go to the Access page to find the login instructions. Access your Cloud Shell at https://shell.cloud.ibm.com and login to your remote cluster. Set a CLUSTERNAME environment variable to the name of your cluster. export CLUSTERNAME = remkohdev-iks116-2n-cluster-labs Login and download the cluster configuration, ibmcloud login ibmcloud ks cluster config --cluster $CLUSTERNAME 01. Setup MongoDB \u00b6 We will begin by deploying a MongoDB database and a HelloWorld application that stores messages into the MongoDB. I will use Helm to deploy and configure a Bitnami/MongoDB chart. Go to Deploy MongoDB to setup MongoDB for our HelloWorld app. 02. Deploy a HelloWorld App \u00b6 Now, you have a MongoDB installed, follow the instructions to Deploy a HelloWorld App with ClusterIP . Services \u00b6 Now we can start with the actual lab on Kubernetes Networking. When we created the Java Spring Boot App called HelloWorld , we created a Deployment. The deployment created also a ReplicaSet with 1 replica of the pods. Because we did not create a Service for the helloworld containers running in pods, they cannot yet be accessed. When a Pod is deployed to a worker node, it is assigned a private IP address in the 172.30.0.0/16 range. Worker nodes and pods can securely communicate on the private network by using private IP addresses. However, because Kubernetes creates and destroys Pods dynamically, the location of the Pods, and thus the private IP addresses, change. With a Service object, you can use built-in Kubernetes service discovery to expose Pods. A Service defines a set of Pods and a policy to access those Pods. Kubernetes assigns a single DNS name for a set of Pods and can load balance requests across Pods. When you create a Service, a set of pods and EndPoints are created to manage access to the pods. The Endpoints object in Kubernetes is the list of IP and port addresses to the Pods and are created automatically when a Service is created and are configured with the pods matching the selector defined in the Service. A Service can be configured without a selector, in that case Kubernetes does not create an associated Endpoints object. Let's look at the declaration of the service.yaml , $ cat service.yaml apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld The spec defines a few important attributes, especially selector and ports . The set of Pods that a Service targets, is determined by the selector using labels. When a Service has no selector, the corresponding Endpoints object is not created automatically. This can be useful in cases where you want to define an Endpoint manually, for instance in the case of an external database instance. The Service maps the incoming port to a targetPort of the Deployment. By default the targetPort is set to the same value as the incoming port field. A port definition in Pods can also be given a name, and you can reference these names in the targetPort attribute of the Service instead of using a port number. In the Service example of helloworld , the deployment.yaml file should then have the corresponding port defined that the Service references by name, $ cat deployment.yaml ports: - name: http-server containerPort: 8080 ServiceTypes \u00b6 Before you create a Service for the helloworld application, let's first understand what types of services exist. Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default type is ClusterIP . To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. ServiceType values and their behaviors are: ClusterIP : Exposes the Service on a cluster-internal IP. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is also automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting <Node-Public-IP>:<NodePort> . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. The NodePort and ClusterIP Services, to which the external load balancer routes, are also automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record. An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead, e.g. apiVersion : v1 kind : Service metadata : name : my-database-svc namespace : prod spec : type : ExternalName externalName : my.database.example.com In this example, when you request the service my-database-svc.prod.svc.cluster.local on the cluster, the cluster DNS Service returns a CNAME record for my.database.example.com . You can also use Ingress in place of Service to expose HTTP/HTTPS Services. Ingress however is technically not a ServiceType, but it acts as the entry point for your cluster and lets you consolidate routing rules into a single resource. 03. ClusterIP \u00b6 Add a Service to helloworld \u00b6 Now you have a simple understanding of the different ServiceTypes on Kubernetes, it is time to expose the Deployment of helloworld using a Service. Create the Service object with the default type, you already created the Service resource file when you deployed the HelloWorld app in the previous step. $ kubectl create -f service.yaml service/helloworld created Describe the Service, $ kubectl describe svc helloworld Name: helloworld Namespace: default Labels: app = helloworld Annotations: <none> Selector: app = helloworld Type: ClusterIP IP: 172 .21.161.255 Port: <unset> 8080 /TCP TargetPort: http-server/TCP Endpoints: 172 .30.153.79:8080 Session Affinity: None Events: <none> You see that Kubernetes by default creates a Service of type ClusterIP . The service is now available and discoverable, but only within the cluster. Get the endpoints that were created as part of the Service, $ kubectl get endpoints helloworld NAME ENDPOINTS AGE helloworld 172 .30.153.79:8080 43s To review the full endpoints resource, use the kubectl edit command, but don't make any changes. $ kubectl edit endpoints helloworld apiVersion: v1 kind: Endpoints metadata: name: helloworld namespace: default labels: app: helloworld subsets: - addresses: - ip: 172 .30.153.79 targetRef: kind: Pod name: helloworld-5f8b6b587b-lwvcs Enter <ESC> + :q to exit the commandline vim editor. The Endpoints object now maps the Service object to the Pod on an internal IP address, so that other pods can access the Service of our HelloWorld application. 04. NodePort \u00b6 The HelloWorld Service is accessible now but only within the cluster. To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. Apps inside the cluster can access a pod by using the in-cluster IP of the service or by sending a request to the name of the service. When you use the name of the service, kube-proxy looks up the name in the cluster DNS provider and routes the request to the in-cluster IP address of the service. To allow external traffic into a kubernetes cluster, you need a NodePort ServiceType. If you set the type field of Service to NodePort , Kubernetes allocates a port in the range 30000-32767. Each node proxies the assigned NodePort (the same port number on every Node) into your Service. Patch the existing Service for helloworld to type: NodePort , $ kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"NodePort\"}}' service/helloworld patched Describe the Service again, $ kubectl describe svc helloworld Name: helloworld Namespace: default Labels: app = helloworld Annotations: <none> Selector: app = helloworld Type: NodePort IP: 172 .21.161.255 Port: <unset> 8080 /TCP TargetPort: http-server/TCP NodePort: <unset> 31777 /TCP Endpoints: 172 .30.153.79:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> In this example, Kubernetes added a NodePort with port value 31777 in this example. For everyone, this is likely to be a different port in the range 30000-32767. You can now connect to the service via the public IP address of any worker node in the cluster and traffic gets forwarded to the service, which uses service discovery and the selector of the Service to deliver the request to the assigned pod. With this piece in place we now have a complete pipeline for load balancing external client requests to all the nodes in the cluster. You don't have sufficient permissions to retrieve the cluster's worker nodes' Public IPs for the account that the clusters were created on, via the cloud shell and the ibmcloud cli. But you can find the worker nodes of your cluster in the cluster detail page in the IBM Cloud UI. Find your cluster via https://cloud.ibm.com/kubernetes/clusters . Select your cluster and go to Worker Nodes , look for the Public IP of one of the worker nodes. If you do have sufficient permissions you can view the worker nodes of your cluster with the command, ibmcloud ks worker ls --cluster $CLUSTERNAME Test the deployment, e.g. with the example values for PUBLICIP and NODEPORT, again be aware that the PUBLIC_IP and NODEPORT are different for each of you. $ PUBLICIP = 150 .238.93.100 $ NODEPORT = 31110 $ curl \"http:// $PUBLICIP : $NODEPORT /api\" Welcome to Spring Boot App $ curl \"http:// $PUBLICIP : $NODEPORT /api/hello?name=John\" { \"message\" : \"Hello John\" } $ curl \"http:// $PUBLICIP : $NODEPORT /api/messages\" [{ \"id\" : \"5edda3befc271d2b0330b8a6\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" }] The client connects to the load balancer via a public IP address on the worker node. The load balancer selects a node and connects to it. Kube-proxy receives this connection and forwards it to the service at the cluster IP. At this point the request matches the netfilter rules and gets redirected to the server pod. 05. Loadbalancer \u00b6 In the previous steps, you created a service for the helloworld application with a clusterIP and then added a NodePort to the Service. But you still want a load balancer of some kind in front of your application, whether your clients are internal or coming in over the public network. A load balancer acts as a reverse proxy and distributes network or application traffic across a number of servers. To use a load balancer as a reverse proxy for distributing client traffic to the nodes in a cluster, you need a public IP address for the service that the clients can connect to, and you need IP addresses on the nodes themselves to which the load balancer can forward the requests. A service of type LoadBalancer has all the capabilities of a NodePort service but also the ability to build out a complete ingress path. A LoadBalancer also has some limitations: you cannot configure the load balancer to terminate https traffic, do virtual hosts or path-based routing, so you can\u2019t use a single load balancer to proxy to multiple services. These limitations led to the addition of a separate kubernetes resource for configuring load balancers, called an Ingress (see next section). On cloud providers that support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service. The actual creation of the load balancer happens asynchronously with the Service, so you might have to wait until the load balancer has been created. Load Balancer on IKS \u00b6 The LoadBalancer service type is implemented differently depending on your cluster's infrastructure provider. On IKS (IBM Kubernetes Service), a classic cluster implements a Network Load Balancer (NLB) 1.0 by default. Load Balancing Methods \u00b6 Before we create the load balancer with NLB v1.0 + subdomain for the helloworld application, review the different Load Balancing Methods on IKS: NodePort exposes the app via a port and public IP address on a worker node. NLB v1.0 + subdomain uses basic load balancing that exposes the app with an IP address or a subdomain. NLB v2.0 + subdomain , uses Direct Server Return (DSR) load balancing, which does not change the packets but the destination address, and exposes the app with an IP address or a subdomain, supports SSL termination. (Network load balancer (NLB) 2.0 is in beta.) Istio + NLB subdomain uses basic load balancing that exposes the app with a subdomain and uses Istio routing rules. Ingress with public ALB (Application Load Balancing) uses HTTPS load balancing that exposes the app with a subdomain and uses custom routing rules and SSL termination for multiple apps. You can customize the ALB routing rules with annotations (See next section). Custom Ingress + NLB subdomain uses HTTPS load balancing with a custom Ingress that exposes the app with the IBM-provided ALB subdomain and uses custom routing rules. Create a Network Load Balancer v1.0 \u00b6 In the previous lab, you already created a NodePort Service. Patch the service for helloworld and change the type to LoadBalancer . $ kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' service/helloworld patched If your cluster has more than 1 worker node, a LoadBalancer will be created and an external IP address is assigned to access the service. $ kubectl get svc helloworld NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.161.255 169 .48.67.163 8080 :31777/TCP 24m The biggest difference you can see is the presence of an External IP address for the service now. Or describe the helloworld Service, kubectl describe svc helloworld A Service of type LoadBalancer was created, 1 of 4 available portable public IP addresses were assigned to the Service. When you create a standard cluster on IBM Cloud, IKS automatically provisions a portable public subnet and a portable private subnet. Now to access the NLB for the Service of the helloworld from the internet, you can use the public IP address of the NLB and the assigned port of the service in the format <External_IP_Address>:<NodePort> . NodePorts are accessible on every public and private IP address of every worker node within the cluster. Access the helloworld app in a browser or with Curl, $ EXTERNALIP = 169 .48.67.163 $ curl \"http:// $EXTERNALIP : $NODEPORT /api/hello?name=MeAgain\" { \"message\" : \"Hello MeAgain\" } $ curl \"http:// $EXTERNALIP : $NODEPORT /api/messages\" [{ \"id\" : \"5ee6e6206e986548955388ee\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" } , { \"id\" : \"5ee6e72e6e986548955388ef\" , \"sender\" : \"MeAgain\" , \" message\" : \"Hello MeAgain\" }] 06. Ingress \u00b6 Ingress is technically not a Service, but a load balancer and router for container clusters. It is a Kubernetes API object that manages external access to the services in a cluster. You can use Ingress to expose multiple app services to a public or private network by using a unique public or private route. The Ingress API also supports TLS termination, virtual hosts, and path-based routing. When you create a standard cluster, an Ingress subdomain is already registered by default for your cluster, see the previous step. The paths to your app services are appended to this public route of the default Ingress Subdomain. In a standard cluster on IKS, the Ingress Application Load Balancer (ALB) implements the NGINX Ingress controller. NGINX is one of the more popular load balancers and reverse proxies. To expose an app using Ingress, you must define an Ingress resource first. The Ingress resource is a Kubernetes resource that defines the rules for how to route incoming requests for apps. One Ingress resource is required per namespace where you have apps that you want to expose. Changes to HelloWorld for Ingress \u00b6 I want to access HelloWorld via the Ingress subdomain and a path rule via a path /hello . You need the Ingress Subdomain and Ingress Secret of your cluster to configure your Ingress resource. Go to your cluster's Overview page to see the Ingress subdomain and secret. The Ingress Secret will be the first part of the Ingress Subdomain of your cluster. E.g. if the Ingress Subdomain is the following: remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Then the Ingress Secret will be, remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 The Ingress Subdomain has a format like clustername-<hash>.region.containers.appdomain.cloud . If you have account management permissions, you can use the ibmcloud command in the CLI to retrieve the Ingress Subdomain and Secret, $ ibmcloud ks nlb-dns ls --cluster $CLUSTERNAME OK Hostname IP ( s ) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud 169 .48.67.162 None created remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 default Or, ibmcloud ks cluster get --show-resources -c $CLUSTERNAME Once you have the Ingress Subdomain and the Ingress Secret, create an Ingress resource with the following command, using a rewrite path annotation. In the file below, make sure to change the hosts and host to the Ingress Subdomain of your cluster, and change the secretName to the value Ingress Secret of your cluster. $ echo 'apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: ingress.bluemix.net/rewrite-path: serviceName=helloworld rewrite=/ spec: tls: - hosts: - <Ingress Subdomain> secretName: <Ingress Secret> rules: - host: <Ingress Subdomain> http: paths: - path: /hello backend: serviceName: helloworld servicePort: 8080' > ingress.yaml Make sure you changed the values for hosts , secretName and host , edit the ingress.yaml file to make the necessary changes, vi ingress.yaml Then create the Ingress for helloworld, $ kubectl create -f ingress.yaml ingress.extensions/helloworld-ingress created The above resource will create a similar access path to helloworld as https://remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud/hello . You can further customize Ingres routing with annotations to customize the ALB settings, TLS settings, request and response annocations, service limits, user authentication, or error actions etc.. Try to access the helloworld API and the proxy using the Ingress Subdomain with the path to the service, $ HOST = remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud $ curl \"http:// $HOST /hello/api/hello?name=JaneDoe\" { \"message\" : \"Hello JaneDoe\" } $ curl \"http:// $HOST /hello/api/messages\" [{ \"id\" : \"5ee27306040cd720fa7ba32d\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" } , { \"id\" : \"5ee274e7040cd720fa7ba32e\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5ee2f325040cd720fa7ba32f\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" }] If you instead want to use subdomain paths instead of URI paths, you would add the subdomain prefix to the hosts attribute. We skip creating subdomain paths, but review the followig example. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : helloworld-ingress spec : tls : - hosts : - remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud secretName : remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0000 rules : - host : >- hello.remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud http : paths : - backend : serviceName : helloworld servicePort : 8080 - host : >- helloproxy.remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud http : paths : - backend : serviceName : helloworld-proxy servicePort : 8080 This Ingress resource will create an access path to app1 at https://hello.remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud/ 06. Network Policy \u00b6 In this last section of the Kubernetes Networking lab, I want to create a Network Policy that only allows traffic coming from the HelloWorld application in the default namespace and not from another HelloWorld application in a test namespace. So first, create a new instance of the HelloWorld application in a new namespace called test . Create a new namespace test , kubectl create namespace test Create a second deployment of HelloWorld in the test namespace connecting to the same MongoDB service in the default namespace. Create the Kubernetes deployment resource, $ echo 'apiVersion: apps/v1 kind: Deployment metadata: name: helloworld2 namespace: test labels: app: helloworld2 spec: replicas: 1 selector: matchLabels: app: helloworld2 template: metadata: labels: app: helloworld2 spec: containers: - name: helloworld2 image: remkohdev/helloworld:lab1v1.0 ports: - name: http-server containerPort: 8080' > deployment2.yaml Create the kubernetes Service resource in the test namespace, $ echo 'apiVersion: v1 kind: Service metadata: name: helloworld2 namespace: test labels: app: helloworld2 spec: type: LoadBalancer ports: - port: 8080 targetPort: http-server selector: app: helloworld2 ' > service2.yaml Deploy the Kubernetes Deployment and Service resource for HellWorld2 in the test namespace using the shared MongoDB instance in the default namespace, kubectl create -f deployment2.yaml kubectl create -f service2.yaml Get the External IP and the NodePort for the new service, $ kubectl get svc helloworld2 -n test NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld2 LoadBalancer 172 .21.176.80 169 .48.215.4 8080 :31429/TCP 34s Test the connection to the MongoDB instance, $ EXTERNALIP2 = 169 .48.215.4 $ NODEPORT2 = 31429 $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/messages [{ \"id\" : \"5edda3befc271d2b0330b8a6\" , \"sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5edda81e9a848c335b5d1e40\" , \"sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5eddac2e9a848c335b5d1e41\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5eddb2269a848c335b5d1e42\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" }] Get a new message, $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/hello?name = Joe2 { \"message\" : \"Hello Joe2\" } Network Policy \u00b6 A network policy is a specification of how pods are allowed to communicate with each other and other network endpoints. By default pods are allowed to communicate with all pods, they are non-isolated . But there are conditions where you want pods to reject traffic. NetworkPolicy uses labels to select pods and define rules which specify what traffic is allowed. A policyTypes field in the NetworkPolicy specification includes either Ingress , Egress , or both. Ingress means coming in , and egress means going out . Each rule in ingress/egress allows traffic which matches both the from/to and ports sections. There are four kinds of selectors that group pods in an ingress from section or egress to section: podSelector, namespaceSelector, podSelector and namespaceSelector, ipBlock for IP CIDR ranges. By grouping pods by pod name, namespace, or IP, you can allow and reject incoming and outgoing traffic on a cluster. When you create a Kubernetes cluster on IBM Cloud, default network policies are set up to secure the public network interface of every worker node in the cluster. Every IBM Cloud Kubernetes Service (IKS) cluster is set up with a network plug-in called Calico . You can use both Kubernetes and Calico to create network policies for a cluster. Project Calico is an open source networking and network security project for containers, Kubernetes, OpenShift but also Istio among other. Some of the advantages are that it can enforce a policy at different layers, at the host networking layer or at the service mesh layer for instance; it uses Linux kernel's optimized forwarding and access control capabilities; Calico is interoperable between Kubernetes and non-Kubernetes, in public cloud, on-prem on VMs or bare metal servers; Calico supports the Kubernetes API as well as extended network policy capabilities; and of course at IBM we love Calico because we love open source! ;-) When a Kubernetes network policy is applied, it is automatically converted into a Calico network policy so that Calico can apply it as an Iptables rule. Calico network policies are a superset of the Kubernetes network policies and are applied by using calicoctl commands. Calico enforces the policies. Calico uses two policy resources: NetworkPolicy and GlobalNetworkPolicy resources. A network policy is a namespaced resource that applies to pods/containers/VMs in that namespace. apiVersion : projectcalico.org/v3 kind : NetworkPolicy Calico global network policy is a non-namespaced resource and can be applied to any kind of endpoint (pods, VMs, host interfaces) independent of namespace. apiVersion : projectcalico.org/v3 kind : GlobalNetworkPolicy To create Calico NetworkPolicy or GlobalNetworkPolicy resources, you must install the calicoctl CLI. That takes a little bit too much time for this lab, so no Calico labs will be included in this hands-on lab. To block all traffic from and to the test namespace, create the following Network Policy, $ echo 'kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: test-deny namespace: test spec: podSelector: matchLabels: {} policyTypes: - Ingress - Egress' > deny-test-namespace.yaml Then create the Kubernetes Network Policy, $ kubectl create -f deny-test-namespace.yaml networkpolicy.networking.k8s.io/test-deny created Remember that when a Kubernetes network policy is created, it is automatically converted into a Calico network policy. Try to access the HelloWorld2 API in the test namespace, via the LoadBalancer Service details for External IP and NodePort, $ kubectl get svc helloworld2 -n test $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/messages curl: ( 7 ) Failed to connect to 169 .48.67.164 port 32401 : Connection timed out The service in the test namespace fails because it tries to access the MongoDB instance in the default namespace, but that is not allowed. Now, try to access the HelloWorld API in the default namespace, by using the helloworld service details in the default namespace. $ kubectl get svc helloworld $ curl http:// $EXTERNALIP : $NODEPORT /api/messages [{ \"id\" : \"5edda3befc271d2b0330b8a6\" , \"sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5edda81e9a848c335b5d1e40\" , \" sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5eddac2e9a848c335b5d1e41\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5eddb2269a848c335b5d1e42\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" } , { \"id\" : \"5edeee2a2e61 fc739ece0820\" , \"sender\" : \"Joe2\" , \"message\" : \"Hello Joe2\" }] Delete the test-deny Network Policy again, kubectl delete NetworkPolicy test-deny -n test Now your traffic from helloworld2 to the default namespace is allowed again. $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/messages [{ \"id\" : \"5ee6e6206e986548955388ee\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" } , { \"id\" : \"5ee6e72e6e986548955388ef\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5ee6e9c56e986548955388f0\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" } , { \"id\" : \"5ee6eb4574416c3c675a01a2\" , \"sender\" : \"Joe2\" , \"message\" : \"Hello Joe2\" }] Or to explicitly allow all traffic again, create the following NetworkPolicy, $ echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: - {} policyTypes: - Ingress' > allow-all.yaml Create the new NetworkPolicy again, kubectl create -f allow-all.yaml Delete Resources \u00b6 To delete helloworld2 resources, delete the namespace test , kubectl delete namespace test To delete the NetworkPolicy, kubectl delete networkpolicy test-deny kubectl delete networkpolicy allow-all-ingress To delete the Ingress resource, kubectl delete ingress helloworld-ingress To clean up the created resources for the helloworld application run the following command, kubectl delete deploy helloworld kubectl delete svc helloworld Uninstall MongoDB helm uninstall mongodb Continue to Lab 2 or go back to the Summary .","title":"Lab 1. Kubernetes Networking"},{"location":"lab-01/#lab-01-kubernetes-networking-using-service-types-ingress-and-network-policies-to-control-application-access","text":"Self: https://github.com/remkohdev/kubernetes-networking In this lab we will use several methods to control access to applications on a Kubernetes cluster. We will go through Kubernetes Networking principles and apply different types of Service resources: ClusterIP, NodePort and LoadBalancer. We will also implement an Ingress resource and a Network Policy, while we briefly will talk about Calico policies as well. You need an IBM account to follow the lab. See the Account Setup and Cluster Access In the first steps, you access your cluster, deploy a MongoDB instance and a HelloWorld application, which will take about 15 minutes. During the remainder of the lab, you will create different Service types, an Ingress and a Network Policy.","title":"Lab 01 - Kubernetes Networking, using Service Types, Ingress and Network Policies to Control Application Access"},{"location":"lab-01/#00-access-your-cluster","text":"Follow the instructions at Account Setup and Cluster Access to claim a pre-created cluster. Go to the Access page to find the login instructions. Access your Cloud Shell at https://shell.cloud.ibm.com and login to your remote cluster. Set a CLUSTERNAME environment variable to the name of your cluster. export CLUSTERNAME = remkohdev-iks116-2n-cluster-labs Login and download the cluster configuration, ibmcloud login ibmcloud ks cluster config --cluster $CLUSTERNAME","title":"00. Access Your Cluster"},{"location":"lab-01/#01-setup-mongodb","text":"We will begin by deploying a MongoDB database and a HelloWorld application that stores messages into the MongoDB. I will use Helm to deploy and configure a Bitnami/MongoDB chart. Go to Deploy MongoDB to setup MongoDB for our HelloWorld app.","title":"01. Setup MongoDB"},{"location":"lab-01/#02-deploy-a-helloworld-app","text":"Now, you have a MongoDB installed, follow the instructions to Deploy a HelloWorld App with ClusterIP .","title":"02. Deploy a HelloWorld App"},{"location":"lab-01/#services","text":"Now we can start with the actual lab on Kubernetes Networking. When we created the Java Spring Boot App called HelloWorld , we created a Deployment. The deployment created also a ReplicaSet with 1 replica of the pods. Because we did not create a Service for the helloworld containers running in pods, they cannot yet be accessed. When a Pod is deployed to a worker node, it is assigned a private IP address in the 172.30.0.0/16 range. Worker nodes and pods can securely communicate on the private network by using private IP addresses. However, because Kubernetes creates and destroys Pods dynamically, the location of the Pods, and thus the private IP addresses, change. With a Service object, you can use built-in Kubernetes service discovery to expose Pods. A Service defines a set of Pods and a policy to access those Pods. Kubernetes assigns a single DNS name for a set of Pods and can load balance requests across Pods. When you create a Service, a set of pods and EndPoints are created to manage access to the pods. The Endpoints object in Kubernetes is the list of IP and port addresses to the Pods and are created automatically when a Service is created and are configured with the pods matching the selector defined in the Service. A Service can be configured without a selector, in that case Kubernetes does not create an associated Endpoints object. Let's look at the declaration of the service.yaml , $ cat service.yaml apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld The spec defines a few important attributes, especially selector and ports . The set of Pods that a Service targets, is determined by the selector using labels. When a Service has no selector, the corresponding Endpoints object is not created automatically. This can be useful in cases where you want to define an Endpoint manually, for instance in the case of an external database instance. The Service maps the incoming port to a targetPort of the Deployment. By default the targetPort is set to the same value as the incoming port field. A port definition in Pods can also be given a name, and you can reference these names in the targetPort attribute of the Service instead of using a port number. In the Service example of helloworld , the deployment.yaml file should then have the corresponding port defined that the Service references by name, $ cat deployment.yaml ports: - name: http-server containerPort: 8080","title":"Services"},{"location":"lab-01/#servicetypes","text":"Before you create a Service for the helloworld application, let's first understand what types of services exist. Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default type is ClusterIP . To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. ServiceType values and their behaviors are: ClusterIP : Exposes the Service on a cluster-internal IP. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is also automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting <Node-Public-IP>:<NodePort> . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. The NodePort and ClusterIP Services, to which the external load balancer routes, are also automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record. An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead, e.g. apiVersion : v1 kind : Service metadata : name : my-database-svc namespace : prod spec : type : ExternalName externalName : my.database.example.com In this example, when you request the service my-database-svc.prod.svc.cluster.local on the cluster, the cluster DNS Service returns a CNAME record for my.database.example.com . You can also use Ingress in place of Service to expose HTTP/HTTPS Services. Ingress however is technically not a ServiceType, but it acts as the entry point for your cluster and lets you consolidate routing rules into a single resource.","title":"ServiceTypes"},{"location":"lab-01/#03-clusterip","text":"","title":"03. ClusterIP"},{"location":"lab-01/#add-a-service-to-helloworld","text":"Now you have a simple understanding of the different ServiceTypes on Kubernetes, it is time to expose the Deployment of helloworld using a Service. Create the Service object with the default type, you already created the Service resource file when you deployed the HelloWorld app in the previous step. $ kubectl create -f service.yaml service/helloworld created Describe the Service, $ kubectl describe svc helloworld Name: helloworld Namespace: default Labels: app = helloworld Annotations: <none> Selector: app = helloworld Type: ClusterIP IP: 172 .21.161.255 Port: <unset> 8080 /TCP TargetPort: http-server/TCP Endpoints: 172 .30.153.79:8080 Session Affinity: None Events: <none> You see that Kubernetes by default creates a Service of type ClusterIP . The service is now available and discoverable, but only within the cluster. Get the endpoints that were created as part of the Service, $ kubectl get endpoints helloworld NAME ENDPOINTS AGE helloworld 172 .30.153.79:8080 43s To review the full endpoints resource, use the kubectl edit command, but don't make any changes. $ kubectl edit endpoints helloworld apiVersion: v1 kind: Endpoints metadata: name: helloworld namespace: default labels: app: helloworld subsets: - addresses: - ip: 172 .30.153.79 targetRef: kind: Pod name: helloworld-5f8b6b587b-lwvcs Enter <ESC> + :q to exit the commandline vim editor. The Endpoints object now maps the Service object to the Pod on an internal IP address, so that other pods can access the Service of our HelloWorld application.","title":"Add a Service to helloworld"},{"location":"lab-01/#04-nodeport","text":"The HelloWorld Service is accessible now but only within the cluster. To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. Apps inside the cluster can access a pod by using the in-cluster IP of the service or by sending a request to the name of the service. When you use the name of the service, kube-proxy looks up the name in the cluster DNS provider and routes the request to the in-cluster IP address of the service. To allow external traffic into a kubernetes cluster, you need a NodePort ServiceType. If you set the type field of Service to NodePort , Kubernetes allocates a port in the range 30000-32767. Each node proxies the assigned NodePort (the same port number on every Node) into your Service. Patch the existing Service for helloworld to type: NodePort , $ kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"NodePort\"}}' service/helloworld patched Describe the Service again, $ kubectl describe svc helloworld Name: helloworld Namespace: default Labels: app = helloworld Annotations: <none> Selector: app = helloworld Type: NodePort IP: 172 .21.161.255 Port: <unset> 8080 /TCP TargetPort: http-server/TCP NodePort: <unset> 31777 /TCP Endpoints: 172 .30.153.79:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> In this example, Kubernetes added a NodePort with port value 31777 in this example. For everyone, this is likely to be a different port in the range 30000-32767. You can now connect to the service via the public IP address of any worker node in the cluster and traffic gets forwarded to the service, which uses service discovery and the selector of the Service to deliver the request to the assigned pod. With this piece in place we now have a complete pipeline for load balancing external client requests to all the nodes in the cluster. You don't have sufficient permissions to retrieve the cluster's worker nodes' Public IPs for the account that the clusters were created on, via the cloud shell and the ibmcloud cli. But you can find the worker nodes of your cluster in the cluster detail page in the IBM Cloud UI. Find your cluster via https://cloud.ibm.com/kubernetes/clusters . Select your cluster and go to Worker Nodes , look for the Public IP of one of the worker nodes. If you do have sufficient permissions you can view the worker nodes of your cluster with the command, ibmcloud ks worker ls --cluster $CLUSTERNAME Test the deployment, e.g. with the example values for PUBLICIP and NODEPORT, again be aware that the PUBLIC_IP and NODEPORT are different for each of you. $ PUBLICIP = 150 .238.93.100 $ NODEPORT = 31110 $ curl \"http:// $PUBLICIP : $NODEPORT /api\" Welcome to Spring Boot App $ curl \"http:// $PUBLICIP : $NODEPORT /api/hello?name=John\" { \"message\" : \"Hello John\" } $ curl \"http:// $PUBLICIP : $NODEPORT /api/messages\" [{ \"id\" : \"5edda3befc271d2b0330b8a6\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" }] The client connects to the load balancer via a public IP address on the worker node. The load balancer selects a node and connects to it. Kube-proxy receives this connection and forwards it to the service at the cluster IP. At this point the request matches the netfilter rules and gets redirected to the server pod.","title":"04. NodePort"},{"location":"lab-01/#05-loadbalancer","text":"In the previous steps, you created a service for the helloworld application with a clusterIP and then added a NodePort to the Service. But you still want a load balancer of some kind in front of your application, whether your clients are internal or coming in over the public network. A load balancer acts as a reverse proxy and distributes network or application traffic across a number of servers. To use a load balancer as a reverse proxy for distributing client traffic to the nodes in a cluster, you need a public IP address for the service that the clients can connect to, and you need IP addresses on the nodes themselves to which the load balancer can forward the requests. A service of type LoadBalancer has all the capabilities of a NodePort service but also the ability to build out a complete ingress path. A LoadBalancer also has some limitations: you cannot configure the load balancer to terminate https traffic, do virtual hosts or path-based routing, so you can\u2019t use a single load balancer to proxy to multiple services. These limitations led to the addition of a separate kubernetes resource for configuring load balancers, called an Ingress (see next section). On cloud providers that support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service. The actual creation of the load balancer happens asynchronously with the Service, so you might have to wait until the load balancer has been created.","title":"05. Loadbalancer"},{"location":"lab-01/#load-balancer-on-iks","text":"The LoadBalancer service type is implemented differently depending on your cluster's infrastructure provider. On IKS (IBM Kubernetes Service), a classic cluster implements a Network Load Balancer (NLB) 1.0 by default.","title":"Load Balancer on IKS"},{"location":"lab-01/#load-balancing-methods","text":"Before we create the load balancer with NLB v1.0 + subdomain for the helloworld application, review the different Load Balancing Methods on IKS: NodePort exposes the app via a port and public IP address on a worker node. NLB v1.0 + subdomain uses basic load balancing that exposes the app with an IP address or a subdomain. NLB v2.0 + subdomain , uses Direct Server Return (DSR) load balancing, which does not change the packets but the destination address, and exposes the app with an IP address or a subdomain, supports SSL termination. (Network load balancer (NLB) 2.0 is in beta.) Istio + NLB subdomain uses basic load balancing that exposes the app with a subdomain and uses Istio routing rules. Ingress with public ALB (Application Load Balancing) uses HTTPS load balancing that exposes the app with a subdomain and uses custom routing rules and SSL termination for multiple apps. You can customize the ALB routing rules with annotations (See next section). Custom Ingress + NLB subdomain uses HTTPS load balancing with a custom Ingress that exposes the app with the IBM-provided ALB subdomain and uses custom routing rules.","title":"Load Balancing Methods"},{"location":"lab-01/#create-a-network-load-balancer-v10","text":"In the previous lab, you already created a NodePort Service. Patch the service for helloworld and change the type to LoadBalancer . $ kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' service/helloworld patched If your cluster has more than 1 worker node, a LoadBalancer will be created and an external IP address is assigned to access the service. $ kubectl get svc helloworld NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.161.255 169 .48.67.163 8080 :31777/TCP 24m The biggest difference you can see is the presence of an External IP address for the service now. Or describe the helloworld Service, kubectl describe svc helloworld A Service of type LoadBalancer was created, 1 of 4 available portable public IP addresses were assigned to the Service. When you create a standard cluster on IBM Cloud, IKS automatically provisions a portable public subnet and a portable private subnet. Now to access the NLB for the Service of the helloworld from the internet, you can use the public IP address of the NLB and the assigned port of the service in the format <External_IP_Address>:<NodePort> . NodePorts are accessible on every public and private IP address of every worker node within the cluster. Access the helloworld app in a browser or with Curl, $ EXTERNALIP = 169 .48.67.163 $ curl \"http:// $EXTERNALIP : $NODEPORT /api/hello?name=MeAgain\" { \"message\" : \"Hello MeAgain\" } $ curl \"http:// $EXTERNALIP : $NODEPORT /api/messages\" [{ \"id\" : \"5ee6e6206e986548955388ee\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" } , { \"id\" : \"5ee6e72e6e986548955388ef\" , \"sender\" : \"MeAgain\" , \" message\" : \"Hello MeAgain\" }]","title":"Create a Network Load Balancer v1.0"},{"location":"lab-01/#06-ingress","text":"Ingress is technically not a Service, but a load balancer and router for container clusters. It is a Kubernetes API object that manages external access to the services in a cluster. You can use Ingress to expose multiple app services to a public or private network by using a unique public or private route. The Ingress API also supports TLS termination, virtual hosts, and path-based routing. When you create a standard cluster, an Ingress subdomain is already registered by default for your cluster, see the previous step. The paths to your app services are appended to this public route of the default Ingress Subdomain. In a standard cluster on IKS, the Ingress Application Load Balancer (ALB) implements the NGINX Ingress controller. NGINX is one of the more popular load balancers and reverse proxies. To expose an app using Ingress, you must define an Ingress resource first. The Ingress resource is a Kubernetes resource that defines the rules for how to route incoming requests for apps. One Ingress resource is required per namespace where you have apps that you want to expose.","title":"06. Ingress"},{"location":"lab-01/#changes-to-helloworld-for-ingress","text":"I want to access HelloWorld via the Ingress subdomain and a path rule via a path /hello . You need the Ingress Subdomain and Ingress Secret of your cluster to configure your Ingress resource. Go to your cluster's Overview page to see the Ingress subdomain and secret. The Ingress Secret will be the first part of the Ingress Subdomain of your cluster. E.g. if the Ingress Subdomain is the following: remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Then the Ingress Secret will be, remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 The Ingress Subdomain has a format like clustername-<hash>.region.containers.appdomain.cloud . If you have account management permissions, you can use the ibmcloud command in the CLI to retrieve the Ingress Subdomain and Secret, $ ibmcloud ks nlb-dns ls --cluster $CLUSTERNAME OK Hostname IP ( s ) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud 169 .48.67.162 None created remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 default Or, ibmcloud ks cluster get --show-resources -c $CLUSTERNAME Once you have the Ingress Subdomain and the Ingress Secret, create an Ingress resource with the following command, using a rewrite path annotation. In the file below, make sure to change the hosts and host to the Ingress Subdomain of your cluster, and change the secretName to the value Ingress Secret of your cluster. $ echo 'apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: ingress.bluemix.net/rewrite-path: serviceName=helloworld rewrite=/ spec: tls: - hosts: - <Ingress Subdomain> secretName: <Ingress Secret> rules: - host: <Ingress Subdomain> http: paths: - path: /hello backend: serviceName: helloworld servicePort: 8080' > ingress.yaml Make sure you changed the values for hosts , secretName and host , edit the ingress.yaml file to make the necessary changes, vi ingress.yaml Then create the Ingress for helloworld, $ kubectl create -f ingress.yaml ingress.extensions/helloworld-ingress created The above resource will create a similar access path to helloworld as https://remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud/hello . You can further customize Ingres routing with annotations to customize the ALB settings, TLS settings, request and response annocations, service limits, user authentication, or error actions etc.. Try to access the helloworld API and the proxy using the Ingress Subdomain with the path to the service, $ HOST = remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud $ curl \"http:// $HOST /hello/api/hello?name=JaneDoe\" { \"message\" : \"Hello JaneDoe\" } $ curl \"http:// $HOST /hello/api/messages\" [{ \"id\" : \"5ee27306040cd720fa7ba32d\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" } , { \"id\" : \"5ee274e7040cd720fa7ba32e\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5ee2f325040cd720fa7ba32f\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" }] If you instead want to use subdomain paths instead of URI paths, you would add the subdomain prefix to the hosts attribute. We skip creating subdomain paths, but review the followig example. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : helloworld-ingress spec : tls : - hosts : - remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud secretName : remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0000 rules : - host : >- hello.remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud http : paths : - backend : serviceName : helloworld servicePort : 8080 - host : >- helloproxy.remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud http : paths : - backend : serviceName : helloworld-proxy servicePort : 8080 This Ingress resource will create an access path to app1 at https://hello.remkohdev-iks116-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud/","title":"Changes to HelloWorld for Ingress"},{"location":"lab-01/#06-network-policy","text":"In this last section of the Kubernetes Networking lab, I want to create a Network Policy that only allows traffic coming from the HelloWorld application in the default namespace and not from another HelloWorld application in a test namespace. So first, create a new instance of the HelloWorld application in a new namespace called test . Create a new namespace test , kubectl create namespace test Create a second deployment of HelloWorld in the test namespace connecting to the same MongoDB service in the default namespace. Create the Kubernetes deployment resource, $ echo 'apiVersion: apps/v1 kind: Deployment metadata: name: helloworld2 namespace: test labels: app: helloworld2 spec: replicas: 1 selector: matchLabels: app: helloworld2 template: metadata: labels: app: helloworld2 spec: containers: - name: helloworld2 image: remkohdev/helloworld:lab1v1.0 ports: - name: http-server containerPort: 8080' > deployment2.yaml Create the kubernetes Service resource in the test namespace, $ echo 'apiVersion: v1 kind: Service metadata: name: helloworld2 namespace: test labels: app: helloworld2 spec: type: LoadBalancer ports: - port: 8080 targetPort: http-server selector: app: helloworld2 ' > service2.yaml Deploy the Kubernetes Deployment and Service resource for HellWorld2 in the test namespace using the shared MongoDB instance in the default namespace, kubectl create -f deployment2.yaml kubectl create -f service2.yaml Get the External IP and the NodePort for the new service, $ kubectl get svc helloworld2 -n test NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld2 LoadBalancer 172 .21.176.80 169 .48.215.4 8080 :31429/TCP 34s Test the connection to the MongoDB instance, $ EXTERNALIP2 = 169 .48.215.4 $ NODEPORT2 = 31429 $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/messages [{ \"id\" : \"5edda3befc271d2b0330b8a6\" , \"sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5edda81e9a848c335b5d1e40\" , \"sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5eddac2e9a848c335b5d1e41\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5eddb2269a848c335b5d1e42\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" }] Get a new message, $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/hello?name = Joe2 { \"message\" : \"Hello Joe2\" }","title":"06. Network Policy"},{"location":"lab-01/#network-policy","text":"A network policy is a specification of how pods are allowed to communicate with each other and other network endpoints. By default pods are allowed to communicate with all pods, they are non-isolated . But there are conditions where you want pods to reject traffic. NetworkPolicy uses labels to select pods and define rules which specify what traffic is allowed. A policyTypes field in the NetworkPolicy specification includes either Ingress , Egress , or both. Ingress means coming in , and egress means going out . Each rule in ingress/egress allows traffic which matches both the from/to and ports sections. There are four kinds of selectors that group pods in an ingress from section or egress to section: podSelector, namespaceSelector, podSelector and namespaceSelector, ipBlock for IP CIDR ranges. By grouping pods by pod name, namespace, or IP, you can allow and reject incoming and outgoing traffic on a cluster. When you create a Kubernetes cluster on IBM Cloud, default network policies are set up to secure the public network interface of every worker node in the cluster. Every IBM Cloud Kubernetes Service (IKS) cluster is set up with a network plug-in called Calico . You can use both Kubernetes and Calico to create network policies for a cluster. Project Calico is an open source networking and network security project for containers, Kubernetes, OpenShift but also Istio among other. Some of the advantages are that it can enforce a policy at different layers, at the host networking layer or at the service mesh layer for instance; it uses Linux kernel's optimized forwarding and access control capabilities; Calico is interoperable between Kubernetes and non-Kubernetes, in public cloud, on-prem on VMs or bare metal servers; Calico supports the Kubernetes API as well as extended network policy capabilities; and of course at IBM we love Calico because we love open source! ;-) When a Kubernetes network policy is applied, it is automatically converted into a Calico network policy so that Calico can apply it as an Iptables rule. Calico network policies are a superset of the Kubernetes network policies and are applied by using calicoctl commands. Calico enforces the policies. Calico uses two policy resources: NetworkPolicy and GlobalNetworkPolicy resources. A network policy is a namespaced resource that applies to pods/containers/VMs in that namespace. apiVersion : projectcalico.org/v3 kind : NetworkPolicy Calico global network policy is a non-namespaced resource and can be applied to any kind of endpoint (pods, VMs, host interfaces) independent of namespace. apiVersion : projectcalico.org/v3 kind : GlobalNetworkPolicy To create Calico NetworkPolicy or GlobalNetworkPolicy resources, you must install the calicoctl CLI. That takes a little bit too much time for this lab, so no Calico labs will be included in this hands-on lab. To block all traffic from and to the test namespace, create the following Network Policy, $ echo 'kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: test-deny namespace: test spec: podSelector: matchLabels: {} policyTypes: - Ingress - Egress' > deny-test-namespace.yaml Then create the Kubernetes Network Policy, $ kubectl create -f deny-test-namespace.yaml networkpolicy.networking.k8s.io/test-deny created Remember that when a Kubernetes network policy is created, it is automatically converted into a Calico network policy. Try to access the HelloWorld2 API in the test namespace, via the LoadBalancer Service details for External IP and NodePort, $ kubectl get svc helloworld2 -n test $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/messages curl: ( 7 ) Failed to connect to 169 .48.67.164 port 32401 : Connection timed out The service in the test namespace fails because it tries to access the MongoDB instance in the default namespace, but that is not allowed. Now, try to access the HelloWorld API in the default namespace, by using the helloworld service details in the default namespace. $ kubectl get svc helloworld $ curl http:// $EXTERNALIP : $NODEPORT /api/messages [{ \"id\" : \"5edda3befc271d2b0330b8a6\" , \"sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5edda81e9a848c335b5d1e40\" , \" sender\" : \"john\" , \"message\" : \"Hello john\" } , { \"id\" : \"5eddac2e9a848c335b5d1e41\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5eddb2269a848c335b5d1e42\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" } , { \"id\" : \"5edeee2a2e61 fc739ece0820\" , \"sender\" : \"Joe2\" , \"message\" : \"Hello Joe2\" }] Delete the test-deny Network Policy again, kubectl delete NetworkPolicy test-deny -n test Now your traffic from helloworld2 to the default namespace is allowed again. $ curl http:// $EXTERNALIP2 : $NODEPORT2 /api/messages [{ \"id\" : \"5ee6e6206e986548955388ee\" , \"sender\" : \"John\" , \"message\" : \"Hello John\" } , { \"id\" : \"5ee6e72e6e986548955388ef\" , \"sender\" : \"MeAgain\" , \"message\" : \"Hello MeAgain\" } , { \"id\" : \"5ee6e9c56e986548955388f0\" , \"sender\" : \"JaneDoe\" , \"message\" : \"Hello JaneDoe\" } , { \"id\" : \"5ee6eb4574416c3c675a01a2\" , \"sender\" : \"Joe2\" , \"message\" : \"Hello Joe2\" }] Or to explicitly allow all traffic again, create the following NetworkPolicy, $ echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: - {} policyTypes: - Ingress' > allow-all.yaml Create the new NetworkPolicy again, kubectl create -f allow-all.yaml","title":"Network Policy"},{"location":"lab-01/#delete-resources","text":"To delete helloworld2 resources, delete the namespace test , kubectl delete namespace test To delete the NetworkPolicy, kubectl delete networkpolicy test-deny kubectl delete networkpolicy allow-all-ingress To delete the Ingress resource, kubectl delete ingress helloworld-ingress To clean up the created resources for the helloworld application run the following command, kubectl delete deploy helloworld kubectl delete svc helloworld Uninstall MongoDB helm uninstall mongodb Continue to Lab 2 or go back to the Summary .","title":"Delete Resources"},{"location":"lab-01/CALICO/","text":"Tips for using Calico \u00b6 $ wget https://github.com/projectcalico/calicoctl/releases/download/v3.14.1/calicoctl $ chmod 755 calicoctl $ echo 'export PATH= \\(HOME:\\) PATH' > .bash_profile $ source .bash_profile $ ibmcloud ks cluster config --cluster remkohdev-iks116-2n-cluster --admin --network find your cloud shell name $ ls -al /tmp/ic .. cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 $ SHELL=cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 Find the cluster's admin folder $ ls -al /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/ .. remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0 remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin $ ADMINDIR=remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin /tmp/ic/ \\(SHELL/.bluemix/plugins/container-service/clusters/\\) ADMINDIR/calicoctl.cfg $ mv /tmp/ic/ \\(SHELL/.bluemix/plugins/container-service/clusters/\\) ADMINDIR/calicoctl.cfg /etc/calico $ calicoctl get nodes NAME kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 $ calicoctl version Client Version: v3.10.0 Git commit: 7968b525 Cluster Version: v3.9.5 Cluster Type: k8s,bgp View network policy $ calicoctl get hostendpoint -o yaml apiVersion: projectcalico.org/v3 items: apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:51Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-private resourceVersion: \"2643\" uid: e0eaf0d6-a36b-11ea-8f29-06ce448b0c2d spec: expectedIPs: 10.187.222.149 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:39Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-public resourceVersion: \"2642\" uid: 78ec7195-2ddd-4b5f-b7c4-8a54d25f4beb spec: expectedIPs: 150.238.93.101 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:01Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-private resourceVersion: \"2295\" uid: c2fcd831-a36b-11ea-97f2-06edddc7e672 spec: expectedIPs: 10.187.222.146 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:22:54Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-public resourceVersion: \"2294\" uid: e8c1fb2b-1a88-4cb4-a10e-67c9bdd28e1c spec: expectedIPs: 150.238.93.100 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 kind: HostEndpointList metadata: resourceVersion: \"40605\" $ calicoctl get NetworkPolicy --all-namespaces -o wide NAMESPACE NAME ORDER SELECTOR kube-system knp.default.kubernetes-dashboard 1000 projectcalico.org/orchestrator == 'k8s' && k8s-app == 'kubernetes-dashboard' $ calicoctl get GlobalNetworkPolicy -o wide NAME ORDER SELECTOR allow-all-outbound 1900 ibm.role in { 'worker_public', 'master_public' } allow-all-private-default 100000 ibm.role == 'worker_private' allow-bigfix-port 1900 ibm.role in { 'worker_public', 'master_public' } allow-icmp 1500 ibm.role in { 'worker_public', 'master_public' } allow-node-port-dnat 1500 ibm.role == 'worker_public' allow-sys-mgmt 1950 ibm.role in { 'worker_public', 'master_public' } allow-vrrp 1500 ibm.role == 'worker_public'","title":"Tips for using Calico"},{"location":"lab-01/CALICO/#tips-for-using-calico","text":"$ wget https://github.com/projectcalico/calicoctl/releases/download/v3.14.1/calicoctl $ chmod 755 calicoctl $ echo 'export PATH= \\(HOME:\\) PATH' > .bash_profile $ source .bash_profile $ ibmcloud ks cluster config --cluster remkohdev-iks116-2n-cluster --admin --network find your cloud shell name $ ls -al /tmp/ic .. cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 $ SHELL=cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 Find the cluster's admin folder $ ls -al /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/ .. remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0 remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin $ ADMINDIR=remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin /tmp/ic/ \\(SHELL/.bluemix/plugins/container-service/clusters/\\) ADMINDIR/calicoctl.cfg $ mv /tmp/ic/ \\(SHELL/.bluemix/plugins/container-service/clusters/\\) ADMINDIR/calicoctl.cfg /etc/calico $ calicoctl get nodes NAME kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 $ calicoctl version Client Version: v3.10.0 Git commit: 7968b525 Cluster Version: v3.9.5 Cluster Type: k8s,bgp View network policy $ calicoctl get hostendpoint -o yaml apiVersion: projectcalico.org/v3 items: apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:51Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-private resourceVersion: \"2643\" uid: e0eaf0d6-a36b-11ea-8f29-06ce448b0c2d spec: expectedIPs: 10.187.222.149 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:39Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-public resourceVersion: \"2642\" uid: 78ec7195-2ddd-4b5f-b7c4-8a54d25f4beb spec: expectedIPs: 150.238.93.101 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:01Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-private resourceVersion: \"2295\" uid: c2fcd831-a36b-11ea-97f2-06edddc7e672 spec: expectedIPs: 10.187.222.146 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:22:54Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-public resourceVersion: \"2294\" uid: e8c1fb2b-1a88-4cb4-a10e-67c9bdd28e1c spec: expectedIPs: 150.238.93.100 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 kind: HostEndpointList metadata: resourceVersion: \"40605\" $ calicoctl get NetworkPolicy --all-namespaces -o wide NAMESPACE NAME ORDER SELECTOR kube-system knp.default.kubernetes-dashboard 1000 projectcalico.org/orchestrator == 'k8s' && k8s-app == 'kubernetes-dashboard' $ calicoctl get GlobalNetworkPolicy -o wide NAME ORDER SELECTOR allow-all-outbound 1900 ibm.role in { 'worker_public', 'master_public' } allow-all-private-default 100000 ibm.role == 'worker_private' allow-bigfix-port 1900 ibm.role in { 'worker_public', 'master_public' } allow-icmp 1500 ibm.role in { 'worker_public', 'master_public' } allow-node-port-dnat 1500 ibm.role == 'worker_public' allow-sys-mgmt 1950 ibm.role in { 'worker_public', 'master_public' } allow-vrrp 1500 ibm.role == 'worker_public'","title":"Tips for using Calico"},{"location":"lab-01/CLEANUP/","text":"Cleanup script \u00b6 kubectl delete namespace test kubectl delete networkpolicy test-deny -n test kubectl delete networkpolicy allow-all-ingress kubectl delete ingress helloworld-ingress kubectl delete deploy helloworld kubectl delete svc helloworld helm uninstall mongodb","title":"Cleanup script"},{"location":"lab-01/CLEANUP/#cleanup-script","text":"kubectl delete namespace test kubectl delete networkpolicy test-deny -n test kubectl delete networkpolicy allow-all-ingress kubectl delete ingress helloworld-ingress kubectl delete deploy helloworld kubectl delete svc helloworld helm uninstall mongodb","title":"Cleanup script"},{"location":"lab-01/CLUSTERADMIN/","text":"Kubernetes Networking \u00b6 Before we start and create our own Ingress object, let's review some of the network management tasks on an IBM Cloud Kubernetes (IKS) service. Note that for these labs you will not have full account permissions, so you do not have sufficient permissions to execute all the commands, but please follow along with the presentor's demo or review of the commands listed here. These commands are just extra information that will help you manage an account and clusters on IBM Cloud. When you created a Service of type LoadBalancer, a NodePort was created as well. To access the application via the service NodePort, you can get the public IP address of the worker nodes and the NodePort of the Service. With a LoadBalancer type Service, you can access the service via an External IP, but the pods still are accessible via the public IP of the worker node and the NodePort as well as via the External IP of the NLB, unless you restrict access to your nodes. List Service details, $ kubectl get svc helloworld NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.161.255 169 .48.67.163 8080 :31777/TCP 31m List the worker nodes on the cluster, Note: the following commands require account management permissions, just review the next steps. Retrieve the worker nodes via the following cli command, $ ibmcloud ks worker ls --cluster $CLUSTERNAME OK ID Public IP Private IP Flavor State Status Zone Version kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b 150 .238.93.101 10 .187.222.149 b3c.4x16.encrypted normal Ready dal13 1 .16.10_1533 kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 150 .238.93.100 10 .187.222.146 b3c.4x16.encrypted normal Ready dal13 1 .16.10_1533 You can access the application then via the worker node's public IP address and Service NodePort, e.g. at http://150.238.93.101:31777 . If you created a standard cluster in the very beginning, IKS automatically provisioned a portable public subnet and a portable private subnet for the VLAN. You need account permissions to list all subnets on your account, e.g. $ ibmcloud ks subnets --provider classic 2422910 10 .186.196.112/29 10 .186.196.113 2847992 private br0ktged0io7g05iakcg dal13 1609575 169 .60.156.136/29 169 .60.156.137 2847990 public br0ktged0io7g05iakcg dal13 or to list the resources for the cluster, e.g. $ ibmcloud ks cluster get --show-resources -c $CLUSTERNAME Retrieving cluster remkohdev-iks116-2n-cluster and all its resources... OK Name: remkohdev-iks116-2n-cluster ID: br9v078d0qi43m0e31n0 State: normal Created: 2020 -05-31T17:58:58+0000 Location: dal13 Master URL: https://c108.us-south.containers.cloud.ibm.com:30356 Public Service Endpoint URL: https://c108.us-south.containers.cloud.ibm.com:30356 Private Service Endpoint URL: https://c108.private.us-south.containers.cloud.ibm.com:30356 Master Location: Dallas Master Status: Ready ( 5 hours ago ) Master State: deployed Master Health: normal Ingress Subdomain: remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Ingress Secret: remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 Workers: 2 Worker Zones: dal13 Version: 1 .16.10_1533 Creator: remkohdev@us.ibm.com Monitoring Dashboard: - Resource Group ID: fdd290732f7d47909181a189494e2990 Resource Group Name: default Subnet VLANs VLAN ID Subnet CIDR Public User-managed 2847992 10 .208.29.72/29 false false 2847990 169 .48.67.160/29 true false The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public network load balancer services, or NLBs. To list all of the portable IP addresses in the IKS cluster, both used and available, you can retrieve the following ConfigMap in the kube-system namespace listing the resources of the subnets, $ kubectl get cm ibm-cloud-provider-vlan-ip-config -n kube-system -o yaml apiVersion: v1 kind: ConfigMap data: cluster_id: br9v078d0qi43m0e31n0 reserved_private_ip: \"\" reserved_private_vlan_id: \"\" reserved_public_ip: \"\" reserved_public_vlan_id: \"\" vlanipmap.json: | - { \"vlans\" : [ { \"id\" : \"2847992\" , \"subnets\" : [ { \"id\" : \"2086403\" , \"ips\" : [ \"10.208.29.74\" , \"10.208.29.75\" , \"10.208.29.76\" , \"10.208.29.77\" , \"10.208.29.78\" ] , \"is_public\" : false, \"is_byoip\" : false, \"cidr\" : \"10.208.29.72/29\" } ] , \"zone\" : \"dal13\" , \"region\" : \"us-south\" } , { \"id\" : \"2847990\" , \"subnets\" : [ { \"id\" : \"2387344\" , \"ips\" : [ \"169.48.67.162\" , \"169.48.67.163\" , \"169.48.67.164\" , \"169.48.67.165\" , \"169.48.67.166\" ] , \"is_public\" : true, \"is_byoip\" : false, \"cidr\" : \"169.48.67.160/29\" } ] , \"zone\" : \"dal13\" , \"region\" : \"us-south\" } ] , \"vlan_errors\" : [] , \"reserved_ips\" : [] } One of the public IP addresses on the public VLAN's subnet is assigned to the NLB. To list the registered NLB host names and IP addresses in a cluster, you need account access permissions, $ ibmcloud ks nlb-dns ls --cluster $CLUSTERNAME OK Hostname IP ( s ) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud 169 .48.67.162 None created remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 default This last command also gives you the default Ingress Subdomain and Ingress Secret that IKS already created by default. You can use these in the next section, configuring the Ingress resource. You see that the portable IP address 169.48.67.162 is assigned to the NLB. You can access the application via the portable IP address of the NLB and service NodePort at http://169.48.67.162:31777 .","title":"Kubernetes Networking"},{"location":"lab-01/CLUSTERADMIN/#kubernetes-networking","text":"Before we start and create our own Ingress object, let's review some of the network management tasks on an IBM Cloud Kubernetes (IKS) service. Note that for these labs you will not have full account permissions, so you do not have sufficient permissions to execute all the commands, but please follow along with the presentor's demo or review of the commands listed here. These commands are just extra information that will help you manage an account and clusters on IBM Cloud. When you created a Service of type LoadBalancer, a NodePort was created as well. To access the application via the service NodePort, you can get the public IP address of the worker nodes and the NodePort of the Service. With a LoadBalancer type Service, you can access the service via an External IP, but the pods still are accessible via the public IP of the worker node and the NodePort as well as via the External IP of the NLB, unless you restrict access to your nodes. List Service details, $ kubectl get svc helloworld NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.161.255 169 .48.67.163 8080 :31777/TCP 31m List the worker nodes on the cluster, Note: the following commands require account management permissions, just review the next steps. Retrieve the worker nodes via the following cli command, $ ibmcloud ks worker ls --cluster $CLUSTERNAME OK ID Public IP Private IP Flavor State Status Zone Version kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b 150 .238.93.101 10 .187.222.149 b3c.4x16.encrypted normal Ready dal13 1 .16.10_1533 kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 150 .238.93.100 10 .187.222.146 b3c.4x16.encrypted normal Ready dal13 1 .16.10_1533 You can access the application then via the worker node's public IP address and Service NodePort, e.g. at http://150.238.93.101:31777 . If you created a standard cluster in the very beginning, IKS automatically provisioned a portable public subnet and a portable private subnet for the VLAN. You need account permissions to list all subnets on your account, e.g. $ ibmcloud ks subnets --provider classic 2422910 10 .186.196.112/29 10 .186.196.113 2847992 private br0ktged0io7g05iakcg dal13 1609575 169 .60.156.136/29 169 .60.156.137 2847990 public br0ktged0io7g05iakcg dal13 or to list the resources for the cluster, e.g. $ ibmcloud ks cluster get --show-resources -c $CLUSTERNAME Retrieving cluster remkohdev-iks116-2n-cluster and all its resources... OK Name: remkohdev-iks116-2n-cluster ID: br9v078d0qi43m0e31n0 State: normal Created: 2020 -05-31T17:58:58+0000 Location: dal13 Master URL: https://c108.us-south.containers.cloud.ibm.com:30356 Public Service Endpoint URL: https://c108.us-south.containers.cloud.ibm.com:30356 Private Service Endpoint URL: https://c108.private.us-south.containers.cloud.ibm.com:30356 Master Location: Dallas Master Status: Ready ( 5 hours ago ) Master State: deployed Master Health: normal Ingress Subdomain: remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud Ingress Secret: remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 Workers: 2 Worker Zones: dal13 Version: 1 .16.10_1533 Creator: remkohdev@us.ibm.com Monitoring Dashboard: - Resource Group ID: fdd290732f7d47909181a189494e2990 Resource Group Name: default Subnet VLANs VLAN ID Subnet CIDR Public User-managed 2847992 10 .208.29.72/29 false false 2847990 169 .48.67.160/29 true false The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public network load balancer services, or NLBs. To list all of the portable IP addresses in the IKS cluster, both used and available, you can retrieve the following ConfigMap in the kube-system namespace listing the resources of the subnets, $ kubectl get cm ibm-cloud-provider-vlan-ip-config -n kube-system -o yaml apiVersion: v1 kind: ConfigMap data: cluster_id: br9v078d0qi43m0e31n0 reserved_private_ip: \"\" reserved_private_vlan_id: \"\" reserved_public_ip: \"\" reserved_public_vlan_id: \"\" vlanipmap.json: | - { \"vlans\" : [ { \"id\" : \"2847992\" , \"subnets\" : [ { \"id\" : \"2086403\" , \"ips\" : [ \"10.208.29.74\" , \"10.208.29.75\" , \"10.208.29.76\" , \"10.208.29.77\" , \"10.208.29.78\" ] , \"is_public\" : false, \"is_byoip\" : false, \"cidr\" : \"10.208.29.72/29\" } ] , \"zone\" : \"dal13\" , \"region\" : \"us-south\" } , { \"id\" : \"2847990\" , \"subnets\" : [ { \"id\" : \"2387344\" , \"ips\" : [ \"169.48.67.162\" , \"169.48.67.163\" , \"169.48.67.164\" , \"169.48.67.165\" , \"169.48.67.166\" ] , \"is_public\" : true, \"is_byoip\" : false, \"cidr\" : \"169.48.67.160/29\" } ] , \"zone\" : \"dal13\" , \"region\" : \"us-south\" } ] , \"vlan_errors\" : [] , \"reserved_ips\" : [] } One of the public IP addresses on the public VLAN's subnet is assigned to the NLB. To list the registered NLB host names and IP addresses in a cluster, you need account access permissions, $ ibmcloud ks nlb-dns ls --cluster $CLUSTERNAME OK Hostname IP ( s ) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud 169 .48.67.162 None created remkohdev-iks116-294603-2bef1f4b4097001da9502000c44fc2b2-0000 default This last command also gives you the default Ingress Subdomain and Ingress Secret that IKS already created by default. You can use these in the next section, configuring the Ingress resource. You see that the portable IP address 169.48.67.162 is assigned to the NLB. You can access the application via the portable IP address of the NLB and service NodePort at http://169.48.67.162:31777 .","title":"Kubernetes Networking"},{"location":"lab-01/JAVAAPP/","text":"Deploy a HelloWorld App with ClusterIP \u00b6 Create the Kubernetes deployment resource, $ echo 'apiVersion: apps/v1 kind: Deployment metadata: name: helloworld labels: app: helloworld spec: replicas: 1 selector: matchLabels: app: helloworld template: metadata: labels: app: helloworld spec: containers: - name: helloworld image: remkohdev/helloworld:lab1v1.0 ports: - name: http-server containerPort: 8080' > deployment.yaml Create the kubernetes Service resource, $ echo 'apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld' > service.yaml Deploy the Kubernetes Deployment resource for HellWorld, $ kubectl create -f deployment.yaml deployment.apps/helloworld created Check the deployment, $ kubectl get all NAME READY STATUS RESTARTS AGE pod/helloworld-5f8b6b587b-5tcd7 1 /1 Running 0 4m56s pod/mongodb-867d8f9796-2tqb2 1 /1 Running 0 46m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 7d service/mongodb LoadBalancer 172 .21.97.193 169 .48.67.164 27017 :30001/TCP 46m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 1 /1 1 1 4m56s deployment.apps/mongodb 1 /1 1 1 46m NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-5f8b6b587b 1 1 1 4m56s replicaset.apps/mongodb-867d8f9796 1 1 1 46m Go back to the next step in the lab .","title":"Deploy a HelloWorld App with ClusterIP"},{"location":"lab-01/JAVAAPP/#deploy-a-helloworld-app-with-clusterip","text":"Create the Kubernetes deployment resource, $ echo 'apiVersion: apps/v1 kind: Deployment metadata: name: helloworld labels: app: helloworld spec: replicas: 1 selector: matchLabels: app: helloworld template: metadata: labels: app: helloworld spec: containers: - name: helloworld image: remkohdev/helloworld:lab1v1.0 ports: - name: http-server containerPort: 8080' > deployment.yaml Create the kubernetes Service resource, $ echo 'apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld' > service.yaml Deploy the Kubernetes Deployment resource for HellWorld, $ kubectl create -f deployment.yaml deployment.apps/helloworld created Check the deployment, $ kubectl get all NAME READY STATUS RESTARTS AGE pod/helloworld-5f8b6b587b-5tcd7 1 /1 Running 0 4m56s pod/mongodb-867d8f9796-2tqb2 1 /1 Running 0 46m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 7d service/mongodb LoadBalancer 172 .21.97.193 169 .48.67.164 27017 :30001/TCP 46m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 1 /1 1 1 4m56s deployment.apps/mongodb 1 /1 1 1 46m NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-5f8b6b587b 1 1 1 4m56s replicaset.apps/mongodb-867d8f9796 1 1 1 46m Go back to the next step in the lab .","title":"Deploy a HelloWorld App with ClusterIP"},{"location":"lab-01/MONGODB/","text":"Deploy MongoDB to IKS Cluster and Persist its Datastore in IBM Cloud Object Storage \u00b6 In this section, you are going to deploy an instance of MongoDB to your IKS cluster. We will disable persistence for simplicity. To install the Bitnami/MongoDB chart, you need Helm v3. At the time of writing, by default, Helm v2.16 was installed on the Cloud Shell . In the Cloud Shell , download and unzip Helm v3. wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > .bash_profile source .bash_profile Verify Helm v3 installation. $ helm version --short v3.2.0+ge11b7ce Add the bitnami Helm repository to your repositories and update the local repository. $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"bitnami\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Now, install MongoDB using helm with the following parameters. Note, that I am deliberately opening NodePort for MongoDB for the purpose of the lab. $ helm install mongodb --set persistence.enabled = false,livenessProbe.initialDelaySeconds = 180 ,usePassword = true,mongodbRootPassword = passw0rd,mongodbUsername = user1,mongodbPassword = passw0rd,mongodbDatabase = mydb bitnami/mongodb --set service.type = NodePort,service.nodePort = 30001 NAME: mongodb LAST DEPLOYED: Sat May 23 21 :04:44 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.default.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) To get the password for \"my-user\" run: export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) To connect to your database run the following command: kubectl run --namespace default mongodb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace default svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Note, the service type for MongoDB is set to NodePort with the Helm parameter --set service.type=NodePort , and the nodePort value is set to 30001 . Normally, you will set MongoDB to be accessed only within the cluster using the type ClusterIP . Patch the NodePort to type LoadBalancer for ease of use, cause it will give you an External IP, and it will allow you to do a simple liveness test. $ kubectl patch svc mongodb -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' service/mongodb patched As mentioned before, in a production environment you don't want to allow external access to your database. Retrieve and save MongoDB passwords in environment variables. $ export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) $ export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) $ echo $MONGODB_ROOT_PASSWORD passw0rd $ echo $MONGODB_PASSWORD passw0rd Verify the MongoDB deployment. $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1 /1 1 1 6m30s Note: It may take 1 to 2 minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 Verify that pods are running. $ kubectl get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1 /1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. $ kubectl get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE mongodb LoadBalancer 172 .21.97.193 169 .48.67.164 27017 :30001/TCP 39s try to connect to MongoDB using the EXTERNAL IP and NodePort of your mongodb service, retrieved in the previous step: $ curl http://169.48.67.164:30001 It looks like you are trying to access MongoDB over HTTP on the native driver port. The reply It looks like you are trying to access MongoDB over HTTP on the native driver port. means that you hit the MongoDB service, which in its turn does not allow http access. Go back to the Lab and continue with the next step to deploy the HelloWorld application.","title":"Deploy MongoDB to IKS Cluster and Persist its Datastore in IBM Cloud Object Storage"},{"location":"lab-01/MONGODB/#deploy-mongodb-to-iks-cluster-and-persist-its-datastore-in-ibm-cloud-object-storage","text":"In this section, you are going to deploy an instance of MongoDB to your IKS cluster. We will disable persistence for simplicity. To install the Bitnami/MongoDB chart, you need Helm v3. At the time of writing, by default, Helm v2.16 was installed on the Cloud Shell . In the Cloud Shell , download and unzip Helm v3. wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > .bash_profile source .bash_profile Verify Helm v3 installation. $ helm version --short v3.2.0+ge11b7ce Add the bitnami Helm repository to your repositories and update the local repository. $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"bitnami\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Now, install MongoDB using helm with the following parameters. Note, that I am deliberately opening NodePort for MongoDB for the purpose of the lab. $ helm install mongodb --set persistence.enabled = false,livenessProbe.initialDelaySeconds = 180 ,usePassword = true,mongodbRootPassword = passw0rd,mongodbUsername = user1,mongodbPassword = passw0rd,mongodbDatabase = mydb bitnami/mongodb --set service.type = NodePort,service.nodePort = 30001 NAME: mongodb LAST DEPLOYED: Sat May 23 21 :04:44 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.default.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) To get the password for \"my-user\" run: export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) To connect to your database run the following command: kubectl run --namespace default mongodb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace default svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Note, the service type for MongoDB is set to NodePort with the Helm parameter --set service.type=NodePort , and the nodePort value is set to 30001 . Normally, you will set MongoDB to be accessed only within the cluster using the type ClusterIP . Patch the NodePort to type LoadBalancer for ease of use, cause it will give you an External IP, and it will allow you to do a simple liveness test. $ kubectl patch svc mongodb -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' service/mongodb patched As mentioned before, in a production environment you don't want to allow external access to your database. Retrieve and save MongoDB passwords in environment variables. $ export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) $ export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) $ echo $MONGODB_ROOT_PASSWORD passw0rd $ echo $MONGODB_PASSWORD passw0rd Verify the MongoDB deployment. $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1 /1 1 1 6m30s Note: It may take 1 to 2 minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 Verify that pods are running. $ kubectl get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1 /1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. $ kubectl get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE mongodb LoadBalancer 172 .21.97.193 169 .48.67.164 27017 :30001/TCP 39s try to connect to MongoDB using the EXTERNAL IP and NodePort of your mongodb service, retrieved in the previous step: $ curl http://169.48.67.164:30001 It looks like you are trying to access MongoDB over HTTP on the native driver port. The reply It looks like you are trying to access MongoDB over HTTP on the native driver port. means that you hit the MongoDB service, which in its turn does not allow http access. Go back to the Lab and continue with the next step to deploy the HelloWorld application.","title":"Deploy MongoDB to IKS Cluster and Persist its Datastore in IBM Cloud Object Storage"},{"location":"lab-02/","text":"Lab02 - Adding Secure Encrypted Object Storage using a Persistent Volume for MongoDB with S3FS-Fuse \u00b6 Pre-requisites \u00b6 Before starting the exercise, you need to have an IBM Cloud account an instance of IBM Kubernetes Service ( https://cloud.ibm.com/kubernetes/catalog/create ) an instance of IBM Cloud Object Storage ( https://cloud.ibm.com/catalog/services/cloud-object-storage ) access to the IBM Cloud Shell, Overview of IBM Cloud Object Storage \u00b6 An important part of data security and persistence on Kubernetes depends on physical storage outside the container orchestration engine that Kubernetes is. You can use PersistentVolume and PersistentVolumeClaim to map data directories to external physical storage. But also, data persistence on a stateless platform like Kubernetes should require extra attention. IBM Cloud Object Storage (COS) offers a few exceptional features that help secure data on Kubernetes. IBM Cloud Object Storage (COS) actively participates in several industry compliance programs and provides the following compliance, certifications, attestations, or reports as measure of proof: ISO 27001, PCI-DSS for Payment Card Industry (PCI) USA, HIPAA for Healthcare USA, (including administrative, physical, and technical safeguards required of Business Associates in 45 CFR Part 160 and Subparts A and C of Part 164), ISO 22301 Business Continuity Management, ISO 27017, ISO 27018, ISO 31000 Risk Management Principles, ISO 9001 Quality Management System, SOC1 Type 2 (SSAE 16), (System and Organization Controls 1), SOC2 Type 2 (SSAE 16), (System and Organization Controls 2), CSA STAR Level 1 (Self-Assessment), General Data Protection Regulation (GDPR) ready, Privacy shield certified. At a high level, information on IBM Cloud Object Storage (COS) is encrypted, then dispersed across multiple geographic locations, and accessed over popular protocols like HTTP with a RESTful API. SecureSlice distributes the data in slices across geo locations so that no full copy of data exists on any individual storage node, and automatically encrypts each segment of data before it is erasure coded and dispersed. The content can only be re-assembled through IBM Cloud\u2019s Accesser technology at the client\u2019s primary data center, where the data was originally received, and decrypted again by SecureSlice. Data-in-place or data-at-rest security is ensured when you persist database contents in IBM Cloud Object Storage. You also have a choice to use integration capabilities with IBM Cloud Key Management Services like IBM Key Protect (using FIPS 140-2 Level 3 certified hardware security modules (HSMs)) and Hyper Protect Crypto Services (built on FIPS 140-2 Level 4-certified hardware) for enhanced security features and compliance. Overview of IBM Cloud Object Storage Plugin \u00b6 This lab uses the IBM Cloud Object Storage plugin to connect an encrypted Object Storage to the Kubernetes cluster via PersistentVolume . A MongoDB database is setup that persists its data to a highly encrypted IBM Cloud Object Storage through PersistentVolume. A sample Java Spring Boot application stores its data in the MongoDB database and its data gets encrypted and persisted. IBM Cloud Object Storage plugin is a Kubernetes volume plugin that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. s3fs allows Linux and macOS to mount an S3 bucket via FUSE. IBM Cloud Shell \u00b6 The IBM Cloud Shell is a web-based terminal running as a container on a Kubernetes cluster in the IBM Cloud. It provides common CLIs when you work with container, kubernetes and OpenShift related technologies. The steps in this exercise are written and tested in the Cloud Shell environment. Lab \u00b6 Login to IBM Cloud . Start an instance of Cloud Shell by either clicking its icon at the top-right corner of the screen or using the url https://shell.cloud.ibm.com in a new browser tab. Note: most of steps in this exercise will be performed in Cloud Shell . Important: A Cloud Shell session times out after it's idle for more than 60 minutes. When a Cloud Shell session times out, you'll lose every work that was performed during the session, if it's not persistent. For example, any CLI tool installation in a Cloud Shell will be lost after the session expires. Cloud Shell is a shell environment running in a container. In the Cloud Shell , login to IBM Cloud from the CLI tool. ibmcloud login or if using Single Sign On, ibmcloud login -sso Retieve your cluster information. $ ibmcloud ks clusters Name ID State Created Workers Location Version Resource Group Name Provider yourcluster br78vuhd069a00er8s9g normal 1 day ago 1 Dallas 1 .16.10_1533 default classic For your convenience, store your IKS cluster name in a environment variable CLUSTERNAME for future reference. export CLUSTERNAME = <your cluster name> Connect to your cluster instance. $ ibmcloud ks cluster config --cluster $CLUSTERNAME Added context for leez-iks-1node to the current kubeconfig file. You can now execute 'kubectl' commands against your cluster. For example, run 'kubectl get nodes' . Verify the connection to your cluster. kubectl config current-context kubectl get nodes Installing Helm v3 \u00b6 You are going to install IBM Cloud Object Storage Plugin via Helm v3 CLI. At the time of writing, by default, Helm v2.16 was installed on the Cloud Shell . In the Cloud Shell , download and unzip Helm v3.2. wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > .bash_profile source .bash_profile Verify Helm v3 installation. $ helm version --short v3.2.0+ge11b7ce Preparing IBM Cloud Object Storage Service Instance \u00b6 If you have an existing IBM Cloud Object Storage service instance, you can use it for the remaining of the exercise. If you don't have any IBM Cloud Object Storage service instance or prefer to create a new instance for this exercise, go to https://cloud.ibm.com/catalog/services/cloud-object-storage and create a Lite plan of Cloud Object Storage for free. You can only have 1 single free Lite instance per account. Note, that if you are using a pre-created cluster you are now logged into a different account than your personal account, because the other account is where the clusters were created for you. On this other account you do not have permission to create new services, so switch to your personal account first before you create the new service. If you are using the CLI to create a new service, in the Cloud Shell open a new session, and login to your personal account, You also need a resource group at the time of writing, but none was created when you created a new account recently yet, Check if you already have a resource-group ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you do not have a resource group yet, create one, $ ibmcloud resource group-create Default Creating resource group Default under account 5081ea1988f14a66a3ddf9d7fb3c6b29 as remko@remkoh.dev... OK Resource group Default was created. Resource Group ID: 93f7a4cd3c824c0cbe90d8f21b46f758 Create a new Object Storage instance via CLI command, for the lab you can use a Lite plan. ibmcloud resource service-instance-create <instance-name> cloud-object-storage <plan> global -g Default For example, $ ibmcloud resource service-instance-create cos-securityconference cloud-object-storage Lite global -g Default OK Service instance cos-securityconference was created. Name: cos-securityconference ID: crn:v1:bluemix:public:cloud-object-storage:global:a/ e65910fa61ce9072d64902d03f3d4774:fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7:: GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Location: global State: active Type: service_instance Sub Type: Allow Cleanup: false Locked: false Created at: 2020 -05-29T15:55:26Z Updated at: 2020 -05-29T15:55:26Z Last Operation: Status create succeeded Message Completed create instance operation Now you need to add credentials. You can do this from the CLI, ibmcloud resource service-key-create my-cos-lab2-credentials Writer --instance-name \"cos-securityconference\" --parameters '{\"HMAC\":true}' ibmcloud resource service-key my-cos-lab2-credentials Or via the web UI. In a browser, navigate to https://cloud.ibm.com/resources which shows a list of your services providioned in your cloud account. Expand the Storage section. Locate and select your IBM Cloud Object Storage service instance. Navigate to the Service credentials tab. Click on New credential button. Change the name to reference the Cloud Object Storage, e.g. my-cos-lab2-credentials For Role accept Writer , Accept all other default settings, and select Add to create a new one. Expand your new service credentials, you will need the credentials to configure the persistent volume later, and take a note of apikey in your Service credential and name of your IBM Cloud Object Storage service instance. For your convenience, in the Cloud Shell store information in environment variables, store the Object Storage service name in COS_SERVICE and the credentials apikey in COS_APIKEY . Store each environment variable in cloud shell sessions for both accounts if you are using both your personal account and the pre-created account. In the Cloud Shell , export COS_SERVICE = cos-securityconference export COS_APIKEY = H4pWU7tKDIA0D95xQrDPmjwvA5JB4CuHXbCAn6I6bg5H Note: replace the example values with your own! Retrieve GUID of your IBM Cloud Object Storage service instance. Note, that you should open a separate session in the cloud shell and be logged in to your own personal account. You have to be logged in to the account where the COS instance was created. $ ibmcloud resource service-instance $COS_SERVICE | grep GUID GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 For your convenience, store information in environment variable COS_GUID . export COS_GUID = fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Note: replace the example value with your own GUID. From the Cloud Shell logged in to the account where your cluster was created, create a Kubernetes Secret to store the COS service credentials named cos-write-access . $ kubectl create secret generic cos-write-access --type = ibm/ibmc-s3fs --from-literal = api-key = $COS_APIKEY --from-literal = service-instance-id = $COS_GUID secret/cos-write-access created Installing IBM Cloud Object Storage Plugin \u00b6 You are going to install the IBM Cloud Object Storage Plugin in your cluster, using the Helm CLI tool in this section. In the Cloud Shell with access to your remote cluster, add a Helm repository where IBM Cloud Object Storage Plugin chart resides. $ helm repo add ibm-charts https://icr.io/helm/ibm-charts ` ibm-charts ` has been added to your repositories Refresh your local Helm repository. $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Download and unzip the IBM Cloud Object Storage plugin to your client, then install the plugin to your cluster from local client. $ helm pull --untar ibm-charts/ibm-object-storage-plugin $ ls -al $ helm plugin install ./ibm-object-storage-plugin/helm-ibmc Installed plugin: ibmc Housekeeping to allow execution of the ibmc.sh script by making the file executable. chmod 755 $HOME /.local/share/helm/plugins/helm-ibmc/ibmc.sh Verify the IBM Cloud Object Storage installation. The plugin usage information should be displayed when running the command below. helm ibmc --help Configuring IBM Cloud Object Storage Plugin \u00b6 Before using the IBM Cloud Object Storage Plugin , configuration changes are required. In the Cloud Shell where you downloaded the IBM Cloud Object Storage plugin, navigate to the templates folder of the IBM Cloud Object Storage Plugin installation. cd ibm-object-storage-plugin/templates && ls -al Make sure the provisioner-sa.yaml file is present and configure it to access the COS service using the COS service credentials secret cos-write-access that you created in the previous section. Open file provisioner-sa.yaml in a editor. vi provisioner-sa.yaml Search for content ibmcloud-object-storage-secret-reader in the file. To move to the right section in the file, in the vi editor, Type colon : Type /ibmcloud-object-storage-secret-reader Press <ENTER> key Find the section below in the vi editor. It's a few lines down. rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] #resourceNames: [\"\"] Use the <i> key to change to Insert mode in vim, uncomment the line and change the section to set the secret to cos-write-access and allow access to the COS instance, rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] resourceNames : [ \"cos-write-access\" ] Save the change and quit the vi editor. Press <ESC> key Type :wq Press <ENTER> key Now, install the configured storage classes for IBM Cloud Object Storage , In the Cloud Shell , navigate back to the user root folder. cd $HOME Install the configured storage classes for IBM Cloud Object Storage , which will use the edited template file. $ helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin Helm version: v3.2.0+ge11b7ce Installing the Helm chart... PROVIDER: CLASSIC DC: hou02 Chart: ./ibm-object-storage-plugin NAME: ibm-object-storage-plugin LAST DEPLOYED: Sat May 23 17 :45:25 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing: ibm-object-storage-plugin. Your release is named: ibm-object-storage-plugin Verify that the storage classes are created successfully. $ kubectl get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 43h Verify that plugin pods are in \"Running\" state and indicate READY state of 1/1 : $ kubectl get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-jwbcw 1 /1 Running 0 43h 10 .185.199.31 10 .185.199.31 <none> <none> ibmcloud-object-storage-plugin-654fc7cd86-kcs8n 1 /1 Running 0 43h 172 .30.194.209 10 .185.199.31 <none> <none> If the pods are not READY and indicate 0/1 then wait and re-run the command until the READY state says 1/1 . The installation is successful when one ibmcloud-object-storage-plugin pod and one or more ibmcloud-object-storage-driver pods are in running state. The number of ibmcloud-object-storage-driver pods equals the number of worker nodes in your cluster. All pods must be in a Running state for the plug-in to function properly. If the pods fail, run kubectl describe pod -n kube-system <pod_name> to find the root cause for the failure. Execute the command below until all pods are in Running state with 1/1 . $ kubectl get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-jwbcw 1 /1 Running 0 43h 10 .185.199.31 10 .185.199.31 <none> <none> ibmcloud-object-storage-plugin-654fc7cd86-kcs8n 1 /1 Running 0 43h 172 .30.194.209 10 .185.199.31 <none> <none> Review the Object Storage Configuration \u00b6 IBM Cloud Kubernetes Service provides pre-defined storage classes that you can use to create buckets with a specific configuration. List available storage classes in IBM Cloud Kubernetes Service. kubectl get storageclasses | grep s3 The Lite service plan for Cloud Object Storage includes Regional and Cross Regional resiliency, flexible data classes, and built in security. For the sample application, I will choose the standard and regional options in the ibmc-s3fs-standard-regional storageclass that is typical for web or mobile apps and we don't need cross-regional resilience beyond resilience per zones for our workshop app, but the options to choose for usage strategies and therefor the pricing of storageclasses for the bucket is very granular. Review the detailed IBM Cloud Object Storage bucket configuration for a storage class. $ kubectl describe storageclass ibmc-s3fs-standard-regional Name: ibmc-s3fs-standard-regional IsDefaultClass: No Annotations: meta.helm.sh/release-name = ibm-object-storage-plugin,meta.helm.sh/release-namespace = default Provisioner: ibm.io/ibmc-s3fs Parameters: ibm.io/chunk-size-mb = 16 ,ibm.io/curl-debug = false,ibm.io/debug-level = warn,ibm.io/iam-endpoint = https://iam.bluemix.net,ibm.io/kernel-cache = true,ibm.io/multireq-max = 20 ,ibm.io/object-store-endpoint = NA,ibm.io/object-store-storage-class = NA,ibm.io/parallel-count = 2 ,ibm.io/s3fs-fuse-retry-count = 5 ,ibm.io/stat-cache-size = 100000 ,ibm.io/tls-cipher-suite = AESGCM AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Additional information is available at https://cloud.ibm.com/docs/containers?topic=containers-object_storage#configure_cos . Create Bucket \u00b6 Data in the IBM Cloud Object Storage is stored and organized in so-called buckets . To create a new bucket in your IBM Cloud Object Storage service instance, In the Cloud Shell in the session logged in to the account that owns the Cloud Object Storage instance, assign a name to the new bucket. The bucket name MUST be globally unique in the IBM Cloud. A simple way to ensure this is to use a random hash or your username as part of the name. If the bucket name is not globally unique, the command in the next step will fail. export COS_BUCKET = <username>-bucket-lab2 Create a new bucket. $ ibmcloud cos create-bucket --ibm-service-instance-id $COS_GUID --class Standard --bucket $COS_BUCKET OK Details about bucket <username>-bucket-lab2: Region: us-south Class: Standard Verify the new bucket was created successfully. $ ibmcloud cos list-buckets --ibm-service-instance-id $COS_GUID OK 1 bucket found in your account: Name Date Created <username>-bucket-lab2 May 29 , 2020 at 21 :22:37 Get your object storage configurations, $ ibmcloud cos config list Key Value Last Updated Default Region us-south Download Location /home/remkohdev/Downloads CRN AccessKeyID SecretAccessKey Authentication Method IAM URL Style VHost This will list your default region. To list your bucket's location use $ ibmcloud cos get-bucket-location --bucket $COS_BUCKET OK Details about bucket remkohdev123-bucket-lab2: Region: us-south Class: Standard With your bucket's location, e.g. us-south , you can find your bucket's private endpoint here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#advanced-endpoint-types , OR in the following steps you find it in your Cloud Object Storage's bucket configuration. In a browser, navigate to https://cloud.ibm.com/resources . Expand the Storage section . Locate and select your IBM Cloud Object Storage service instance. In the left menu, select the buckets section Select your new bucket in the Buckets tab. Select the Configuration tab under Buckets iin the left pane. Take note of the Private endpoint. For your convenience, store the information in environment variable. In the Cloud Shell, export PRIVATE_ENDPOINT = s3.private.us-south.cloud-object-storage.appdomain.cloud Note: replace the endpoint with the one that you identied in the previous setp. Create the PersistentVolumeClaim \u00b6 Depending on the settings that you choose in your PVC, you can provision IBM Cloud Object Storage in the following ways: Dynamic provisioning : When you create the PVC, the matching persistent volume (PV) and the bucket in your IBM Cloud Object Storage service instance are automatically created. Static provisioning : You can reference an existing bucket in your IBM Cloud Object Storage service instance in your PVC. When you create the PVC, only the matching PV is automatically created and linked to your existing bucket in IBM Cloud Object Storage. In this exercise, you are going to use an existing bucket when assigning persistant storage to IKS container. In the cloud shell connected to your cluster, create a PersistentVolumeClaim configuration file. Note: Replace the values for: - ibm.io/bucket , - ibm.io/secret-name and - ibm.io/endpoint . If your values are not exactly matching with the bucket name you created, the secret name you created and the private endpoint of your bucket, the PVC will remain in state pending and fail to create. Note: The secret-name should be set to cos-write-access unless you changed the name of the secret we created earlier, Note: ibm.io/endpoint should be set to the output of command echo \"https://$PRIVATE_ENDPOINT\" Create the file first and then edit the file with vi if changes are needed, Create the file, $ echo 'kind: PersistentVolumeClaim apiVersion: v1 metadata: name: my-iks-pvc namespace: default annotations: ibm.io/auto-create-bucket: \"false\" ibm.io/auto-delete-bucket: \"false\" ibm.io/bucket: \"<your-cos-bucket>\" ibm.io/secret-name: \"cos-write-access\" ibm.io/endpoint: \"https://s3.private.us-south.cloud-object-storage.appdomain.cloud\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ibmc-s3fs-standard-regional' > my-iks-pvc.yaml Edit the file and set the right values if changes are still needed, vi my-iks-pvc.yaml Create a PersistentVolumeClaim . $ kubectl apply -f my-iks-pvc.yaml persistentvolumeclaim/my-iks-pvc created Verify the PersistentVolumeClaim and through the PVC also the PersistentVolume or PV was created successfully and that the PVC has STATUS of Bound . $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-iks-pvc Bound pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO ibmc-s3fs-standard-regional 4s Note: If the state of the PVC remains Pending , you can inspect the error for why the PVC remains pending by using the describe command: kubectl describe pvc <pvc_name> . For example, kubectl describe pvc my-iks-pvc . Note: If the state of the PVC stays as Pending , the problem must be resolved before you move to the next step. Verify a new PersistentVolume was also created successfully. $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO Delete Bound default/my-iks-pvc ibmc-s3fs-standard-regional 74s You're now ready to persistly store data on the IBM Cloud Object Storage within your containers in IKS clusters. Deploy MongoDB to IKS Cluster and Persist its Datastore in IBM Cloud Object Storage \u00b6 In this section, you are going to deploy an instance of MongoDB to your IKS cluster and persistly store data on the IBM Cloud Object Storage. We will skip this step, but if you want to configure the MongoDB via a values.yaml file, or want to review the default values of the Helm chart, in the Cloud Shell , download the default values.yaml file from the bitnami/mongodb Helm chart, which is used to configure and deploy the MongoDB Helm chart. In this lab we will overwrite the values from the commandline when we install the chart. wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/mongodb/values.yaml We will skip this step also, but if you want to review the configuration options, open the values.yaml file in a file editor and review the parameters that can be modified during mongdb deployment. In this exercise however, you'll overwrite the default values using Helm command parameters instead of a values.yaml file. Add the bitnami Helm repository. $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Install MongoDB using helm with parameters, the flag persistence.enabled=true will enable storing your data to a PersistentVolume. $ helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set mongodbRootPassword = passw0rd --set mongodbUsername = user1 --set mongodbPassword = passw0rd --set mongodbDatabase = mydb --set service.type = ClusterIP NAME: mongodb LAST DEPLOYED: Sat May 23 21 :04:44 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.default.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) To get the password for \"my-user\" run: export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) To connect to your database run the following command: kubectl run --namespace default mongodb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace default svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Note: if you used the same cluster for lab1 and lab2, then you can uninstall the existing MongoDB instance from lab1 by typing helm uninstall mongodb . Wait a few minutes, to give Kubernetes time to terminate all resources associated with the chart. Note, the service type for MongoDB is set to ClusterIP with the Helm parameter --set service.type=ClusterIP , so that MongoDB can only be accessed within the cluster. Retrieve and save MongoDB passwords in environment variables. $ export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) $ export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) $ echo $MONGODB_ROOT_PASSWORD passw0rd $ echo $MONGODB_PASSWORD passw0rd Verify the MongoDB deployment. $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1 /1 1 1 6m30s Note: It may take several minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 . Verify that pods are running. $ kubectl get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1 /1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. Verify that the internal MongoDB port 27017 within the container is not exposed externally, $ kubectl get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE mongodb ClusterIP 172 .21.131.154 <none> 27017 /TCP 41s Verify MongoDB Deployment \u00b6 To verify MongoDB deployment, In Cloud Shell , retrieve pod ID. $ kubectl get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1 /1 Running 0 5m40s Start an interactive terminal to the pod, you need to use your own unique pod name with the hashes. $ kubectl exec -it <your pod name> bash I have no name!@<your pod name>:/$ Start a MongoDB CLI session. $ mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD MongoDB shell version v4.2.7 connecting to: mongodb://127.0.0.1:27017/?authSource = admin & compressors = disabled & gssapiServiceName = mongodb Implicit session: session { \"id\" : UUID ( \"a638b7d1-d00d-4de2-954b-ee6309c251b2\" ) } MongoDB server version: 4 .2.7 Welcome to the MongoDB shell. For interactive help, type \"help\" . For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user 2020 -05-30T04:27:20.416+0000 I STORAGE [ main ] In File::open () , ::open for '//.mongorc.js' failed with Permission denied > Switch to your database. > use mydb switched to db mydb Authenticate a MongoDB connection. > db . auth ( \"user1\" , \"passw0rd\" ) 1 Create a collection . > db . createCollection ( \"customer\" ) { \"ok\" : 1 } Verify the collection creation. > db . getCollection ( 'customer' ) mydb . customer Create one data entry in MongoDB. > db . customer . insertOne ( { firstName : \"John\" , lastName : \"Smith\" } ) { \"acknowledged\" : true , \"insertedId\" : ObjectId ( \"5ed1e4319bdb52022d624bdf\" ) } Retrieve the data entry in the MongoDB. > db . customer . find ( { lastName : \"Smith\" } ) { \"_id\" : ObjectId ( \"5ed1e4319bdb52022d624bdf\" ), \"firstName\" : \"John\" , \"lastName\" : \"Smith\" } Type exit twice to back to the Cloud Shell . Your mongodb is now saving values, and if your Cloud Object Storage and bucket were configured correctly, your customer information is now securely stored. If you review the bucket in your Object Storage, MongoDB should now be writing its data files to the object storage. Continue to Lab 3 or go back to the Summary .","title":"Lab 2. Adding Secure Storage"},{"location":"lab-02/#lab02-adding-secure-encrypted-object-storage-using-a-persistent-volume-for-mongodb-with-s3fs-fuse","text":"","title":"Lab02 - Adding Secure Encrypted Object Storage using a Persistent Volume for MongoDB with S3FS-Fuse"},{"location":"lab-02/#pre-requisites","text":"Before starting the exercise, you need to have an IBM Cloud account an instance of IBM Kubernetes Service ( https://cloud.ibm.com/kubernetes/catalog/create ) an instance of IBM Cloud Object Storage ( https://cloud.ibm.com/catalog/services/cloud-object-storage ) access to the IBM Cloud Shell,","title":"Pre-requisites"},{"location":"lab-02/#overview-of-ibm-cloud-object-storage","text":"An important part of data security and persistence on Kubernetes depends on physical storage outside the container orchestration engine that Kubernetes is. You can use PersistentVolume and PersistentVolumeClaim to map data directories to external physical storage. But also, data persistence on a stateless platform like Kubernetes should require extra attention. IBM Cloud Object Storage (COS) offers a few exceptional features that help secure data on Kubernetes. IBM Cloud Object Storage (COS) actively participates in several industry compliance programs and provides the following compliance, certifications, attestations, or reports as measure of proof: ISO 27001, PCI-DSS for Payment Card Industry (PCI) USA, HIPAA for Healthcare USA, (including administrative, physical, and technical safeguards required of Business Associates in 45 CFR Part 160 and Subparts A and C of Part 164), ISO 22301 Business Continuity Management, ISO 27017, ISO 27018, ISO 31000 Risk Management Principles, ISO 9001 Quality Management System, SOC1 Type 2 (SSAE 16), (System and Organization Controls 1), SOC2 Type 2 (SSAE 16), (System and Organization Controls 2), CSA STAR Level 1 (Self-Assessment), General Data Protection Regulation (GDPR) ready, Privacy shield certified. At a high level, information on IBM Cloud Object Storage (COS) is encrypted, then dispersed across multiple geographic locations, and accessed over popular protocols like HTTP with a RESTful API. SecureSlice distributes the data in slices across geo locations so that no full copy of data exists on any individual storage node, and automatically encrypts each segment of data before it is erasure coded and dispersed. The content can only be re-assembled through IBM Cloud\u2019s Accesser technology at the client\u2019s primary data center, where the data was originally received, and decrypted again by SecureSlice. Data-in-place or data-at-rest security is ensured when you persist database contents in IBM Cloud Object Storage. You also have a choice to use integration capabilities with IBM Cloud Key Management Services like IBM Key Protect (using FIPS 140-2 Level 3 certified hardware security modules (HSMs)) and Hyper Protect Crypto Services (built on FIPS 140-2 Level 4-certified hardware) for enhanced security features and compliance.","title":"Overview of IBM Cloud Object Storage"},{"location":"lab-02/#overview-of-ibm-cloud-object-storage-plugin","text":"This lab uses the IBM Cloud Object Storage plugin to connect an encrypted Object Storage to the Kubernetes cluster via PersistentVolume . A MongoDB database is setup that persists its data to a highly encrypted IBM Cloud Object Storage through PersistentVolume. A sample Java Spring Boot application stores its data in the MongoDB database and its data gets encrypted and persisted. IBM Cloud Object Storage plugin is a Kubernetes volume plugin that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. s3fs allows Linux and macOS to mount an S3 bucket via FUSE.","title":"Overview of IBM Cloud Object Storage Plugin"},{"location":"lab-02/#ibm-cloud-shell","text":"The IBM Cloud Shell is a web-based terminal running as a container on a Kubernetes cluster in the IBM Cloud. It provides common CLIs when you work with container, kubernetes and OpenShift related technologies. The steps in this exercise are written and tested in the Cloud Shell environment.","title":"IBM Cloud Shell"},{"location":"lab-02/#lab","text":"Login to IBM Cloud . Start an instance of Cloud Shell by either clicking its icon at the top-right corner of the screen or using the url https://shell.cloud.ibm.com in a new browser tab. Note: most of steps in this exercise will be performed in Cloud Shell . Important: A Cloud Shell session times out after it's idle for more than 60 minutes. When a Cloud Shell session times out, you'll lose every work that was performed during the session, if it's not persistent. For example, any CLI tool installation in a Cloud Shell will be lost after the session expires. Cloud Shell is a shell environment running in a container. In the Cloud Shell , login to IBM Cloud from the CLI tool. ibmcloud login or if using Single Sign On, ibmcloud login -sso Retieve your cluster information. $ ibmcloud ks clusters Name ID State Created Workers Location Version Resource Group Name Provider yourcluster br78vuhd069a00er8s9g normal 1 day ago 1 Dallas 1 .16.10_1533 default classic For your convenience, store your IKS cluster name in a environment variable CLUSTERNAME for future reference. export CLUSTERNAME = <your cluster name> Connect to your cluster instance. $ ibmcloud ks cluster config --cluster $CLUSTERNAME Added context for leez-iks-1node to the current kubeconfig file. You can now execute 'kubectl' commands against your cluster. For example, run 'kubectl get nodes' . Verify the connection to your cluster. kubectl config current-context kubectl get nodes","title":"Lab"},{"location":"lab-02/#installing-helm-v3","text":"You are going to install IBM Cloud Object Storage Plugin via Helm v3 CLI. At the time of writing, by default, Helm v2.16 was installed on the Cloud Shell . In the Cloud Shell , download and unzip Helm v3.2. wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > .bash_profile source .bash_profile Verify Helm v3 installation. $ helm version --short v3.2.0+ge11b7ce","title":"Installing Helm v3"},{"location":"lab-02/#preparing-ibm-cloud-object-storage-service-instance","text":"If you have an existing IBM Cloud Object Storage service instance, you can use it for the remaining of the exercise. If you don't have any IBM Cloud Object Storage service instance or prefer to create a new instance for this exercise, go to https://cloud.ibm.com/catalog/services/cloud-object-storage and create a Lite plan of Cloud Object Storage for free. You can only have 1 single free Lite instance per account. Note, that if you are using a pre-created cluster you are now logged into a different account than your personal account, because the other account is where the clusters were created for you. On this other account you do not have permission to create new services, so switch to your personal account first before you create the new service. If you are using the CLI to create a new service, in the Cloud Shell open a new session, and login to your personal account, You also need a resource group at the time of writing, but none was created when you created a new account recently yet, Check if you already have a resource-group ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you do not have a resource group yet, create one, $ ibmcloud resource group-create Default Creating resource group Default under account 5081ea1988f14a66a3ddf9d7fb3c6b29 as remko@remkoh.dev... OK Resource group Default was created. Resource Group ID: 93f7a4cd3c824c0cbe90d8f21b46f758 Create a new Object Storage instance via CLI command, for the lab you can use a Lite plan. ibmcloud resource service-instance-create <instance-name> cloud-object-storage <plan> global -g Default For example, $ ibmcloud resource service-instance-create cos-securityconference cloud-object-storage Lite global -g Default OK Service instance cos-securityconference was created. Name: cos-securityconference ID: crn:v1:bluemix:public:cloud-object-storage:global:a/ e65910fa61ce9072d64902d03f3d4774:fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7:: GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Location: global State: active Type: service_instance Sub Type: Allow Cleanup: false Locked: false Created at: 2020 -05-29T15:55:26Z Updated at: 2020 -05-29T15:55:26Z Last Operation: Status create succeeded Message Completed create instance operation Now you need to add credentials. You can do this from the CLI, ibmcloud resource service-key-create my-cos-lab2-credentials Writer --instance-name \"cos-securityconference\" --parameters '{\"HMAC\":true}' ibmcloud resource service-key my-cos-lab2-credentials Or via the web UI. In a browser, navigate to https://cloud.ibm.com/resources which shows a list of your services providioned in your cloud account. Expand the Storage section. Locate and select your IBM Cloud Object Storage service instance. Navigate to the Service credentials tab. Click on New credential button. Change the name to reference the Cloud Object Storage, e.g. my-cos-lab2-credentials For Role accept Writer , Accept all other default settings, and select Add to create a new one. Expand your new service credentials, you will need the credentials to configure the persistent volume later, and take a note of apikey in your Service credential and name of your IBM Cloud Object Storage service instance. For your convenience, in the Cloud Shell store information in environment variables, store the Object Storage service name in COS_SERVICE and the credentials apikey in COS_APIKEY . Store each environment variable in cloud shell sessions for both accounts if you are using both your personal account and the pre-created account. In the Cloud Shell , export COS_SERVICE = cos-securityconference export COS_APIKEY = H4pWU7tKDIA0D95xQrDPmjwvA5JB4CuHXbCAn6I6bg5H Note: replace the example values with your own! Retrieve GUID of your IBM Cloud Object Storage service instance. Note, that you should open a separate session in the cloud shell and be logged in to your own personal account. You have to be logged in to the account where the COS instance was created. $ ibmcloud resource service-instance $COS_SERVICE | grep GUID GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 For your convenience, store information in environment variable COS_GUID . export COS_GUID = fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Note: replace the example value with your own GUID. From the Cloud Shell logged in to the account where your cluster was created, create a Kubernetes Secret to store the COS service credentials named cos-write-access . $ kubectl create secret generic cos-write-access --type = ibm/ibmc-s3fs --from-literal = api-key = $COS_APIKEY --from-literal = service-instance-id = $COS_GUID secret/cos-write-access created","title":"Preparing IBM Cloud Object Storage Service Instance"},{"location":"lab-02/#installing-ibm-cloud-object-storage-plugin","text":"You are going to install the IBM Cloud Object Storage Plugin in your cluster, using the Helm CLI tool in this section. In the Cloud Shell with access to your remote cluster, add a Helm repository where IBM Cloud Object Storage Plugin chart resides. $ helm repo add ibm-charts https://icr.io/helm/ibm-charts ` ibm-charts ` has been added to your repositories Refresh your local Helm repository. $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Download and unzip the IBM Cloud Object Storage plugin to your client, then install the plugin to your cluster from local client. $ helm pull --untar ibm-charts/ibm-object-storage-plugin $ ls -al $ helm plugin install ./ibm-object-storage-plugin/helm-ibmc Installed plugin: ibmc Housekeeping to allow execution of the ibmc.sh script by making the file executable. chmod 755 $HOME /.local/share/helm/plugins/helm-ibmc/ibmc.sh Verify the IBM Cloud Object Storage installation. The plugin usage information should be displayed when running the command below. helm ibmc --help","title":"Installing IBM Cloud Object Storage Plugin"},{"location":"lab-02/#configuring-ibm-cloud-object-storage-plugin","text":"Before using the IBM Cloud Object Storage Plugin , configuration changes are required. In the Cloud Shell where you downloaded the IBM Cloud Object Storage plugin, navigate to the templates folder of the IBM Cloud Object Storage Plugin installation. cd ibm-object-storage-plugin/templates && ls -al Make sure the provisioner-sa.yaml file is present and configure it to access the COS service using the COS service credentials secret cos-write-access that you created in the previous section. Open file provisioner-sa.yaml in a editor. vi provisioner-sa.yaml Search for content ibmcloud-object-storage-secret-reader in the file. To move to the right section in the file, in the vi editor, Type colon : Type /ibmcloud-object-storage-secret-reader Press <ENTER> key Find the section below in the vi editor. It's a few lines down. rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] #resourceNames: [\"\"] Use the <i> key to change to Insert mode in vim, uncomment the line and change the section to set the secret to cos-write-access and allow access to the COS instance, rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] resourceNames : [ \"cos-write-access\" ] Save the change and quit the vi editor. Press <ESC> key Type :wq Press <ENTER> key Now, install the configured storage classes for IBM Cloud Object Storage , In the Cloud Shell , navigate back to the user root folder. cd $HOME Install the configured storage classes for IBM Cloud Object Storage , which will use the edited template file. $ helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin Helm version: v3.2.0+ge11b7ce Installing the Helm chart... PROVIDER: CLASSIC DC: hou02 Chart: ./ibm-object-storage-plugin NAME: ibm-object-storage-plugin LAST DEPLOYED: Sat May 23 17 :45:25 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing: ibm-object-storage-plugin. Your release is named: ibm-object-storage-plugin Verify that the storage classes are created successfully. $ kubectl get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 43h ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 43h ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 43h Verify that plugin pods are in \"Running\" state and indicate READY state of 1/1 : $ kubectl get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-jwbcw 1 /1 Running 0 43h 10 .185.199.31 10 .185.199.31 <none> <none> ibmcloud-object-storage-plugin-654fc7cd86-kcs8n 1 /1 Running 0 43h 172 .30.194.209 10 .185.199.31 <none> <none> If the pods are not READY and indicate 0/1 then wait and re-run the command until the READY state says 1/1 . The installation is successful when one ibmcloud-object-storage-plugin pod and one or more ibmcloud-object-storage-driver pods are in running state. The number of ibmcloud-object-storage-driver pods equals the number of worker nodes in your cluster. All pods must be in a Running state for the plug-in to function properly. If the pods fail, run kubectl describe pod -n kube-system <pod_name> to find the root cause for the failure. Execute the command below until all pods are in Running state with 1/1 . $ kubectl get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-jwbcw 1 /1 Running 0 43h 10 .185.199.31 10 .185.199.31 <none> <none> ibmcloud-object-storage-plugin-654fc7cd86-kcs8n 1 /1 Running 0 43h 172 .30.194.209 10 .185.199.31 <none> <none>","title":"Configuring IBM Cloud Object Storage Plugin"},{"location":"lab-02/#review-the-object-storage-configuration","text":"IBM Cloud Kubernetes Service provides pre-defined storage classes that you can use to create buckets with a specific configuration. List available storage classes in IBM Cloud Kubernetes Service. kubectl get storageclasses | grep s3 The Lite service plan for Cloud Object Storage includes Regional and Cross Regional resiliency, flexible data classes, and built in security. For the sample application, I will choose the standard and regional options in the ibmc-s3fs-standard-regional storageclass that is typical for web or mobile apps and we don't need cross-regional resilience beyond resilience per zones for our workshop app, but the options to choose for usage strategies and therefor the pricing of storageclasses for the bucket is very granular. Review the detailed IBM Cloud Object Storage bucket configuration for a storage class. $ kubectl describe storageclass ibmc-s3fs-standard-regional Name: ibmc-s3fs-standard-regional IsDefaultClass: No Annotations: meta.helm.sh/release-name = ibm-object-storage-plugin,meta.helm.sh/release-namespace = default Provisioner: ibm.io/ibmc-s3fs Parameters: ibm.io/chunk-size-mb = 16 ,ibm.io/curl-debug = false,ibm.io/debug-level = warn,ibm.io/iam-endpoint = https://iam.bluemix.net,ibm.io/kernel-cache = true,ibm.io/multireq-max = 20 ,ibm.io/object-store-endpoint = NA,ibm.io/object-store-storage-class = NA,ibm.io/parallel-count = 2 ,ibm.io/s3fs-fuse-retry-count = 5 ,ibm.io/stat-cache-size = 100000 ,ibm.io/tls-cipher-suite = AESGCM AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Additional information is available at https://cloud.ibm.com/docs/containers?topic=containers-object_storage#configure_cos .","title":"Review the Object Storage Configuration"},{"location":"lab-02/#create-bucket","text":"Data in the IBM Cloud Object Storage is stored and organized in so-called buckets . To create a new bucket in your IBM Cloud Object Storage service instance, In the Cloud Shell in the session logged in to the account that owns the Cloud Object Storage instance, assign a name to the new bucket. The bucket name MUST be globally unique in the IBM Cloud. A simple way to ensure this is to use a random hash or your username as part of the name. If the bucket name is not globally unique, the command in the next step will fail. export COS_BUCKET = <username>-bucket-lab2 Create a new bucket. $ ibmcloud cos create-bucket --ibm-service-instance-id $COS_GUID --class Standard --bucket $COS_BUCKET OK Details about bucket <username>-bucket-lab2: Region: us-south Class: Standard Verify the new bucket was created successfully. $ ibmcloud cos list-buckets --ibm-service-instance-id $COS_GUID OK 1 bucket found in your account: Name Date Created <username>-bucket-lab2 May 29 , 2020 at 21 :22:37 Get your object storage configurations, $ ibmcloud cos config list Key Value Last Updated Default Region us-south Download Location /home/remkohdev/Downloads CRN AccessKeyID SecretAccessKey Authentication Method IAM URL Style VHost This will list your default region. To list your bucket's location use $ ibmcloud cos get-bucket-location --bucket $COS_BUCKET OK Details about bucket remkohdev123-bucket-lab2: Region: us-south Class: Standard With your bucket's location, e.g. us-south , you can find your bucket's private endpoint here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#advanced-endpoint-types , OR in the following steps you find it in your Cloud Object Storage's bucket configuration. In a browser, navigate to https://cloud.ibm.com/resources . Expand the Storage section . Locate and select your IBM Cloud Object Storage service instance. In the left menu, select the buckets section Select your new bucket in the Buckets tab. Select the Configuration tab under Buckets iin the left pane. Take note of the Private endpoint. For your convenience, store the information in environment variable. In the Cloud Shell, export PRIVATE_ENDPOINT = s3.private.us-south.cloud-object-storage.appdomain.cloud Note: replace the endpoint with the one that you identied in the previous setp.","title":"Create Bucket"},{"location":"lab-02/#create-the-persistentvolumeclaim","text":"Depending on the settings that you choose in your PVC, you can provision IBM Cloud Object Storage in the following ways: Dynamic provisioning : When you create the PVC, the matching persistent volume (PV) and the bucket in your IBM Cloud Object Storage service instance are automatically created. Static provisioning : You can reference an existing bucket in your IBM Cloud Object Storage service instance in your PVC. When you create the PVC, only the matching PV is automatically created and linked to your existing bucket in IBM Cloud Object Storage. In this exercise, you are going to use an existing bucket when assigning persistant storage to IKS container. In the cloud shell connected to your cluster, create a PersistentVolumeClaim configuration file. Note: Replace the values for: - ibm.io/bucket , - ibm.io/secret-name and - ibm.io/endpoint . If your values are not exactly matching with the bucket name you created, the secret name you created and the private endpoint of your bucket, the PVC will remain in state pending and fail to create. Note: The secret-name should be set to cos-write-access unless you changed the name of the secret we created earlier, Note: ibm.io/endpoint should be set to the output of command echo \"https://$PRIVATE_ENDPOINT\" Create the file first and then edit the file with vi if changes are needed, Create the file, $ echo 'kind: PersistentVolumeClaim apiVersion: v1 metadata: name: my-iks-pvc namespace: default annotations: ibm.io/auto-create-bucket: \"false\" ibm.io/auto-delete-bucket: \"false\" ibm.io/bucket: \"<your-cos-bucket>\" ibm.io/secret-name: \"cos-write-access\" ibm.io/endpoint: \"https://s3.private.us-south.cloud-object-storage.appdomain.cloud\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ibmc-s3fs-standard-regional' > my-iks-pvc.yaml Edit the file and set the right values if changes are still needed, vi my-iks-pvc.yaml Create a PersistentVolumeClaim . $ kubectl apply -f my-iks-pvc.yaml persistentvolumeclaim/my-iks-pvc created Verify the PersistentVolumeClaim and through the PVC also the PersistentVolume or PV was created successfully and that the PVC has STATUS of Bound . $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-iks-pvc Bound pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO ibmc-s3fs-standard-regional 4s Note: If the state of the PVC remains Pending , you can inspect the error for why the PVC remains pending by using the describe command: kubectl describe pvc <pvc_name> . For example, kubectl describe pvc my-iks-pvc . Note: If the state of the PVC stays as Pending , the problem must be resolved before you move to the next step. Verify a new PersistentVolume was also created successfully. $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO Delete Bound default/my-iks-pvc ibmc-s3fs-standard-regional 74s You're now ready to persistly store data on the IBM Cloud Object Storage within your containers in IKS clusters.","title":"Create the PersistentVolumeClaim"},{"location":"lab-02/#deploy-mongodb-to-iks-cluster-and-persist-its-datastore-in-ibm-cloud-object-storage","text":"In this section, you are going to deploy an instance of MongoDB to your IKS cluster and persistly store data on the IBM Cloud Object Storage. We will skip this step, but if you want to configure the MongoDB via a values.yaml file, or want to review the default values of the Helm chart, in the Cloud Shell , download the default values.yaml file from the bitnami/mongodb Helm chart, which is used to configure and deploy the MongoDB Helm chart. In this lab we will overwrite the values from the commandline when we install the chart. wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/mongodb/values.yaml We will skip this step also, but if you want to review the configuration options, open the values.yaml file in a file editor and review the parameters that can be modified during mongdb deployment. In this exercise however, you'll overwrite the default values using Helm command parameters instead of a values.yaml file. Add the bitnami Helm repository. $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Install MongoDB using helm with parameters, the flag persistence.enabled=true will enable storing your data to a PersistentVolume. $ helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set mongodbRootPassword = passw0rd --set mongodbUsername = user1 --set mongodbPassword = passw0rd --set mongodbDatabase = mydb --set service.type = ClusterIP NAME: mongodb LAST DEPLOYED: Sat May 23 21 :04:44 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.default.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) To get the password for \"my-user\" run: export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) To connect to your database run the following command: kubectl run --namespace default mongodb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace default svc/mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Note: if you used the same cluster for lab1 and lab2, then you can uninstall the existing MongoDB instance from lab1 by typing helm uninstall mongodb . Wait a few minutes, to give Kubernetes time to terminate all resources associated with the chart. Note, the service type for MongoDB is set to ClusterIP with the Helm parameter --set service.type=ClusterIP , so that MongoDB can only be accessed within the cluster. Retrieve and save MongoDB passwords in environment variables. $ export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) $ export MONGODB_PASSWORD = $( kubectl get secret --namespace default mongodb -o jsonpath = \"{.data.mongodb-password}\" | base64 --decode ) $ echo $MONGODB_ROOT_PASSWORD passw0rd $ echo $MONGODB_PASSWORD passw0rd Verify the MongoDB deployment. $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1 /1 1 1 6m30s Note: It may take several minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 . Verify that pods are running. $ kubectl get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1 /1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. Verify that the internal MongoDB port 27017 within the container is not exposed externally, $ kubectl get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE mongodb ClusterIP 172 .21.131.154 <none> 27017 /TCP 41s","title":"Deploy MongoDB to IKS Cluster and Persist its Datastore in IBM Cloud Object Storage"},{"location":"lab-02/#verify-mongodb-deployment","text":"To verify MongoDB deployment, In Cloud Shell , retrieve pod ID. $ kubectl get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1 /1 Running 0 5m40s Start an interactive terminal to the pod, you need to use your own unique pod name with the hashes. $ kubectl exec -it <your pod name> bash I have no name!@<your pod name>:/$ Start a MongoDB CLI session. $ mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD MongoDB shell version v4.2.7 connecting to: mongodb://127.0.0.1:27017/?authSource = admin & compressors = disabled & gssapiServiceName = mongodb Implicit session: session { \"id\" : UUID ( \"a638b7d1-d00d-4de2-954b-ee6309c251b2\" ) } MongoDB server version: 4 .2.7 Welcome to the MongoDB shell. For interactive help, type \"help\" . For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user 2020 -05-30T04:27:20.416+0000 I STORAGE [ main ] In File::open () , ::open for '//.mongorc.js' failed with Permission denied > Switch to your database. > use mydb switched to db mydb Authenticate a MongoDB connection. > db . auth ( \"user1\" , \"passw0rd\" ) 1 Create a collection . > db . createCollection ( \"customer\" ) { \"ok\" : 1 } Verify the collection creation. > db . getCollection ( 'customer' ) mydb . customer Create one data entry in MongoDB. > db . customer . insertOne ( { firstName : \"John\" , lastName : \"Smith\" } ) { \"acknowledged\" : true , \"insertedId\" : ObjectId ( \"5ed1e4319bdb52022d624bdf\" ) } Retrieve the data entry in the MongoDB. > db . customer . find ( { lastName : \"Smith\" } ) { \"_id\" : ObjectId ( \"5ed1e4319bdb52022d624bdf\" ), \"firstName\" : \"John\" , \"lastName\" : \"Smith\" } Type exit twice to back to the Cloud Shell . Your mongodb is now saving values, and if your Cloud Object Storage and bucket were configured correctly, your customer information is now securely stored. If you review the bucket in your Object Storage, MongoDB should now be writing its data files to the object storage. Continue to Lab 3 or go back to the Summary .","title":"Verify MongoDB Deployment"},{"location":"lab-02/README1/","text":"Create a Java Spring Boot App and Connect to MongoDB \u00b6 See: https://github.com/spring-projects/spring-data-book/blob/master/mongodb See: https://docs.spring.io/spring-data/mongodb/docs/3.0.0.RELEASE/reference/html/#reference spring init --dependencies = web,data-rest,thymeleaf guestbook-api cd guestbook-api mvn clean install mvn test mvn spring-boot:run Create the APIController, $ echo 'package com.example.guestbookapi; import java.util.List; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.RestController; import org.springframework.http.MediaType; import org.springframework.http.ResponseEntity; import org.springframework.http.HttpStatus; @RestController public class APIController { @Autowired private MessageRepository repository; @GetMapping(\"/api\") public String index() { return \"Welcome to Spring Boot App\"; } @RequestMapping(value = \"/api/hello\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public String hello(@RequestParam String name) { String message = \"Hello \"+ name; String responseJson = \"{ \\\"message\\\" : \\\"\"+ message + \"\\\" }\"; repository.save(new Message(name, message)); for (Message msg : repository.findAll()) { System.out.println(msg); } return responseJson; } @RequestMapping(value = \"/api/messages\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public ResponseEntity<List<Message>> getMessages() { List<Message> messages = (List<Message>) repository.findAll(); return new ResponseEntity<List<Message>>(messages, HttpStatus.OK); } } ' > src/main/java/com/example/guestbookapi/APIController.java Create the Message class, $ echo 'package com.example.guestbookapi; import org.springframework.data.annotation.Id; public class Message { @Id private String id; private String sender; private String message; public Message(String sender, String message) { this.sender = sender; this.message = message; } public String getId() { return id; } public String getSender() { return sender; } public String getMessage() { return message; } @Override public String toString() { return \"Message [id=\" + id + \", sender=\" + sender + \", message=\" + message + \"]\"; } }' > src/main/java/com/example/guestbookapi/Message.java Create the MessageRepository class, echo 'package com.example.guestbookapi; import java.util.List; import org.springframework.data.mongodb.repository.MongoRepository; public interface MessageRepository extends MongoRepository<Message, String> { public List<Message> findBySender(String sender); }' > src/main/java/com/example/guestbookapi/MessageRepository.java Add a new file \u2018~/src/main/resources/application.properties\u2019, echo 'spring.data.mongodb.authentication-database=admin spring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=messagesdb spring.data.mongodb.port=27017 spring.data.mongodb.host=mongodb' > src/main/resources/application.properties Test Java App on localhost \u00b6 Create local Mongodb, docker run --name mongo -d -p 27017 :27017 -e MONGO_INITDB_ROOT_USERNAME = user1 -e MONGO_INITDB_ROOT_PASSWORD = passw0rd mongo Get IP address on mac osx, ifconfig en0 | grep inet Configure application.properties and change the host to your local IP Address to test the app on localhost. spring.data.mongodb.authentication-database = admin spring.data.mongodb.username = user1 spring.data.mongodb.password = passw0rd spring.data.mongodb.database = messagesdb spring.data.mongodb.port = 27017 spring.data.mongodb.host = 192.168.1.4 Clean, install and run, mvn clean install mvn spring-boot:run Test it your messages are saved and retrieved, $ curl -X GET 'http://127.0.0.1:8080/api/hello?name=remko' { \"message\" : \"Hello remko\" } $ curl -X GET 'http://127.0.0.1:8080/api/hello?name=emily' { \"message\" : \"Hello emily\" } $ curl --location --request GET 'http://127.0.0.1:8080/api/messages' [{ \"id\" : \"5ec9e4912b3d9d5451fe463a\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko\" } , { \"id\" : \"5ec9e4a52b3d9d5451fe463b\" , \"sender\" : \"emily\" , \"message\" : \"Hello emily\" }] Dockerize \u00b6 FROM openjdk:8-jdk-alpine ARG JAR_FILE = target/*.jar COPY ${ JAR_FILE } app.jar ENTRYPOINT [ \"java\" , \"-jar\" , \"/app.jar\" ] Deploy to Kubernetes \u00b6 TODO: set spring boot config at deploy time via secret, TODO: add Dockerfile, add Deployment files, deploy","title":"Create a Java Spring Boot App and Connect to MongoDB"},{"location":"lab-02/README1/#create-a-java-spring-boot-app-and-connect-to-mongodb","text":"See: https://github.com/spring-projects/spring-data-book/blob/master/mongodb See: https://docs.spring.io/spring-data/mongodb/docs/3.0.0.RELEASE/reference/html/#reference spring init --dependencies = web,data-rest,thymeleaf guestbook-api cd guestbook-api mvn clean install mvn test mvn spring-boot:run Create the APIController, $ echo 'package com.example.guestbookapi; import java.util.List; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.RestController; import org.springframework.http.MediaType; import org.springframework.http.ResponseEntity; import org.springframework.http.HttpStatus; @RestController public class APIController { @Autowired private MessageRepository repository; @GetMapping(\"/api\") public String index() { return \"Welcome to Spring Boot App\"; } @RequestMapping(value = \"/api/hello\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public String hello(@RequestParam String name) { String message = \"Hello \"+ name; String responseJson = \"{ \\\"message\\\" : \\\"\"+ message + \"\\\" }\"; repository.save(new Message(name, message)); for (Message msg : repository.findAll()) { System.out.println(msg); } return responseJson; } @RequestMapping(value = \"/api/messages\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public ResponseEntity<List<Message>> getMessages() { List<Message> messages = (List<Message>) repository.findAll(); return new ResponseEntity<List<Message>>(messages, HttpStatus.OK); } } ' > src/main/java/com/example/guestbookapi/APIController.java Create the Message class, $ echo 'package com.example.guestbookapi; import org.springframework.data.annotation.Id; public class Message { @Id private String id; private String sender; private String message; public Message(String sender, String message) { this.sender = sender; this.message = message; } public String getId() { return id; } public String getSender() { return sender; } public String getMessage() { return message; } @Override public String toString() { return \"Message [id=\" + id + \", sender=\" + sender + \", message=\" + message + \"]\"; } }' > src/main/java/com/example/guestbookapi/Message.java Create the MessageRepository class, echo 'package com.example.guestbookapi; import java.util.List; import org.springframework.data.mongodb.repository.MongoRepository; public interface MessageRepository extends MongoRepository<Message, String> { public List<Message> findBySender(String sender); }' > src/main/java/com/example/guestbookapi/MessageRepository.java Add a new file \u2018~/src/main/resources/application.properties\u2019, echo 'spring.data.mongodb.authentication-database=admin spring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=messagesdb spring.data.mongodb.port=27017 spring.data.mongodb.host=mongodb' > src/main/resources/application.properties","title":"Create a Java Spring Boot App and Connect to MongoDB"},{"location":"lab-02/README1/#test-java-app-on-localhost","text":"Create local Mongodb, docker run --name mongo -d -p 27017 :27017 -e MONGO_INITDB_ROOT_USERNAME = user1 -e MONGO_INITDB_ROOT_PASSWORD = passw0rd mongo Get IP address on mac osx, ifconfig en0 | grep inet Configure application.properties and change the host to your local IP Address to test the app on localhost. spring.data.mongodb.authentication-database = admin spring.data.mongodb.username = user1 spring.data.mongodb.password = passw0rd spring.data.mongodb.database = messagesdb spring.data.mongodb.port = 27017 spring.data.mongodb.host = 192.168.1.4 Clean, install and run, mvn clean install mvn spring-boot:run Test it your messages are saved and retrieved, $ curl -X GET 'http://127.0.0.1:8080/api/hello?name=remko' { \"message\" : \"Hello remko\" } $ curl -X GET 'http://127.0.0.1:8080/api/hello?name=emily' { \"message\" : \"Hello emily\" } $ curl --location --request GET 'http://127.0.0.1:8080/api/messages' [{ \"id\" : \"5ec9e4912b3d9d5451fe463a\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko\" } , { \"id\" : \"5ec9e4a52b3d9d5451fe463b\" , \"sender\" : \"emily\" , \"message\" : \"Hello emily\" }]","title":"Test Java App on localhost"},{"location":"lab-02/README1/#dockerize","text":"FROM openjdk:8-jdk-alpine ARG JAR_FILE = target/*.jar COPY ${ JAR_FILE } app.jar ENTRYPOINT [ \"java\" , \"-jar\" , \"/app.jar\" ]","title":"Dockerize"},{"location":"lab-02/README1/#deploy-to-kubernetes","text":"TODO: set spring boot config at deploy time via secret, TODO: add Dockerfile, add Deployment files, deploy","title":"Deploy to Kubernetes"},{"location":"lab-02/README2/","text":"Optional: Connect to MongoDB within a Java Spring Boot App \u00b6 A sample Java Spring Boot application is provided as part of this repo to demonstrate how you can access a MongoDB database from a Java application. Open cloud-native-security-master/workshop/lab-02/java/src/main/resources/application.properties file in a file editor. Change its contents to reflect your MongoDB connection information. For example, spring.data.mongodb.authentication-database = admin spring.data.mongodb.username = user1 spring.data.mongodb.password = passw0rd spring.data.mongodb.database = mydb spring.data.mongodb.port = 27017 spring.data.mongodb.host = mongodb spring.data.mongodb.uri = mongodb://user1:passw0rd@mongodb:27017/mydb Save the changes. In Cloud Shell , navigate to the sample app folder. cd cloud-native-security-master/workshop/lab-02/java Run the sample application. ./mvnw spring-boot:run If your sample application is executed successfully, you should find the following information toward the end of its output. Start working with Mongo DB Clean up any existing customer data: ------------------------------------ 2020 -05-30 00 :10:04.349 INFO 41598 --- [ main ] org.mongodb.driver.connection : Opened connection [ connectionId { localValue:2, serverValue:660 }] to 184 .172.250.155:30001 Populate customer data: ----------------------- Customers found with findAll () method: -------------------------------------- Customer [ id = 5ed1eaaca03193778dd6521a, firstName = 'Alice' , lastName = 'Smith' ] Customer [ id = 5ed1eaaca03193778dd6521b, firstName = 'Bob' , lastName = 'Smith' ] Customer found with findByFirstName ( 'Alice' ) method: ---------------------------------------------------- Customer [ id = 5ed1eaaca03193778dd6521a, firstName = 'Alice' , lastName = 'Smith' ] Customers found with findByLastName ( 'Smith' ) method: ---------------------------------------------------- Customer [ id = 5ed1eaaca03193778dd6521a, firstName = 'Alice' , lastName = 'Smith' ] Customer [ id = 5ed1eaaca03193778dd6521b, firstName = 'Bob' , lastName = 'Smith' ] End working with Mongo DB Optional Verification \u00b6 You can verify the data entry via the sample Java app through mongoDB shell. Execute the same query as you did in the previous sections, you retrieve two different data entries. > db . customer . find ( { lastName : \"Smith\" } ) { \"_id\" : ObjectId ( \"5ed1eaaca03193778dd6521a\" ), \"firstName\" : \"Alice\" , \"lastName\" : \"Smith\" , \"_class\" : \"com.example.accessingdatamongodb.Customer\" } { \"_id\" : ObjectId ( \"5ed1eaaca03193778dd6521b\" ), \"firstName\" : \"Bob\" , \"lastName\" : \"Smith\" , \"_class\" : \"com.example.accessingdatamongodb.Customer\" } If you are interested in verify the data persistency of the mongoDB database, you may remove the mongoDB from the IKS cluster and install it again. Remove mongodb deployment. helm delete mongodb Re-deploy mongodb to IKS cluster. helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set mongodbRootPassword = passw0rd --set mongodbUsername = user1 --set mongodbPassword = passw0rd --set mongodbDatabase = mydb --set service.type = NodePort --set service.nodePort = 30001 After mongodb redeployment, you can verify the existing data in the mongoDB database via MongoDB shell. Conclusion \u00b6 Congratulation! You have successfully deployed a MongoDB server to IKS cluster and persistant its data in IBM Cloud Object Storage. Becuase the IBM Cloud Object Storage offers data encryption and other rich security features out of box, you achieved data-in-rest objective. There are many options to run a database in Cloud environment. The repo does not suggest one way is better than another. The use case in this repo is to demonstrate data-in-rest via IBM Cloud Object Storage.","title":"Optional: Connect to MongoDB within a Java Spring Boot App"},{"location":"lab-02/README2/#optional-connect-to-mongodb-within-a-java-spring-boot-app","text":"A sample Java Spring Boot application is provided as part of this repo to demonstrate how you can access a MongoDB database from a Java application. Open cloud-native-security-master/workshop/lab-02/java/src/main/resources/application.properties file in a file editor. Change its contents to reflect your MongoDB connection information. For example, spring.data.mongodb.authentication-database = admin spring.data.mongodb.username = user1 spring.data.mongodb.password = passw0rd spring.data.mongodb.database = mydb spring.data.mongodb.port = 27017 spring.data.mongodb.host = mongodb spring.data.mongodb.uri = mongodb://user1:passw0rd@mongodb:27017/mydb Save the changes. In Cloud Shell , navigate to the sample app folder. cd cloud-native-security-master/workshop/lab-02/java Run the sample application. ./mvnw spring-boot:run If your sample application is executed successfully, you should find the following information toward the end of its output. Start working with Mongo DB Clean up any existing customer data: ------------------------------------ 2020 -05-30 00 :10:04.349 INFO 41598 --- [ main ] org.mongodb.driver.connection : Opened connection [ connectionId { localValue:2, serverValue:660 }] to 184 .172.250.155:30001 Populate customer data: ----------------------- Customers found with findAll () method: -------------------------------------- Customer [ id = 5ed1eaaca03193778dd6521a, firstName = 'Alice' , lastName = 'Smith' ] Customer [ id = 5ed1eaaca03193778dd6521b, firstName = 'Bob' , lastName = 'Smith' ] Customer found with findByFirstName ( 'Alice' ) method: ---------------------------------------------------- Customer [ id = 5ed1eaaca03193778dd6521a, firstName = 'Alice' , lastName = 'Smith' ] Customers found with findByLastName ( 'Smith' ) method: ---------------------------------------------------- Customer [ id = 5ed1eaaca03193778dd6521a, firstName = 'Alice' , lastName = 'Smith' ] Customer [ id = 5ed1eaaca03193778dd6521b, firstName = 'Bob' , lastName = 'Smith' ] End working with Mongo DB","title":"Optional: Connect to MongoDB within a Java Spring Boot App"},{"location":"lab-02/README2/#optional-verification","text":"You can verify the data entry via the sample Java app through mongoDB shell. Execute the same query as you did in the previous sections, you retrieve two different data entries. > db . customer . find ( { lastName : \"Smith\" } ) { \"_id\" : ObjectId ( \"5ed1eaaca03193778dd6521a\" ), \"firstName\" : \"Alice\" , \"lastName\" : \"Smith\" , \"_class\" : \"com.example.accessingdatamongodb.Customer\" } { \"_id\" : ObjectId ( \"5ed1eaaca03193778dd6521b\" ), \"firstName\" : \"Bob\" , \"lastName\" : \"Smith\" , \"_class\" : \"com.example.accessingdatamongodb.Customer\" } If you are interested in verify the data persistency of the mongoDB database, you may remove the mongoDB from the IKS cluster and install it again. Remove mongodb deployment. helm delete mongodb Re-deploy mongodb to IKS cluster. helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set mongodbRootPassword = passw0rd --set mongodbUsername = user1 --set mongodbPassword = passw0rd --set mongodbDatabase = mydb --set service.type = NodePort --set service.nodePort = 30001 After mongodb redeployment, you can verify the existing data in the mongoDB database via MongoDB shell.","title":"Optional Verification"},{"location":"lab-02/README2/#conclusion","text":"Congratulation! You have successfully deployed a MongoDB server to IKS cluster and persistant its data in IBM Cloud Object Storage. Becuase the IBM Cloud Object Storage offers data encryption and other rich security features out of box, you achieved data-in-rest objective. There are many options to run a database in Cloud environment. The repo does not suggest one way is better than another. The use case in this repo is to demonstrate data-in-rest via IBM Cloud Object Storage.","title":"Conclusion"},{"location":"lab-02/REMOVE_RESOURCES/","text":"Delete your Resources \u00b6 kubectl delete secret cos-write-access Removing the IBM OS Plugin \u00b6 https://hub.helm.sh/charts/ibm-charts/ibm-object-storage-plugin https://cloud.ibm.com/docs/containers?topic=containers-object_storage Installed in default namespace Verify that you do not have any PVCs or PVs in your cluster that use IBM Cloud Object Storage. List all pods that mount a specific PVC. kubectl get pods --all-namespaces -o = jsonpath = '{range .items[*]}{\"\\n\"}{.metadata.name}{\":\\t\"}{range .spec.volumes[*]}{.persistentVolumeClaim.claimName}{\" \"}{end}{end}' | grep \"<pvc_name>\" If one or more pod is returned, remove the pods or deployment before removing the Helm chart. Find the installation name of your Helm chart. helm ls --all --all-namespaces | grep ibm-object-storage-plugin helm delete <helm_chart_name> -n <helm_chart_namespace> helm delete ibm-object-storage-plugin -n default Remove the ibmc Helm plug-in. Remove the plug-in. helm plugin remove ibmc Verify that the ibmc plug-in is removed. helm plugin list Verify that the IBM Cloud Object Storage pods are removed. kubectl get pods -n <namespace> | grep object-storage Verify that the storage classes are removed. kubectl get storageclasses | grep 'ibmc-s3fs' Remove the PVC \u00b6 kubectl delete pvc my-iks-pvc get pv kubectl get pv delete pv Remove MongoDB \u00b6 helm uninstall mongodb","title":"Delete your Resources"},{"location":"lab-02/REMOVE_RESOURCES/#delete-your-resources","text":"kubectl delete secret cos-write-access","title":"Delete your Resources"},{"location":"lab-02/REMOVE_RESOURCES/#removing-the-ibm-os-plugin","text":"https://hub.helm.sh/charts/ibm-charts/ibm-object-storage-plugin https://cloud.ibm.com/docs/containers?topic=containers-object_storage Installed in default namespace Verify that you do not have any PVCs or PVs in your cluster that use IBM Cloud Object Storage. List all pods that mount a specific PVC. kubectl get pods --all-namespaces -o = jsonpath = '{range .items[*]}{\"\\n\"}{.metadata.name}{\":\\t\"}{range .spec.volumes[*]}{.persistentVolumeClaim.claimName}{\" \"}{end}{end}' | grep \"<pvc_name>\" If one or more pod is returned, remove the pods or deployment before removing the Helm chart. Find the installation name of your Helm chart. helm ls --all --all-namespaces | grep ibm-object-storage-plugin helm delete <helm_chart_name> -n <helm_chart_namespace> helm delete ibm-object-storage-plugin -n default Remove the ibmc Helm plug-in. Remove the plug-in. helm plugin remove ibmc Verify that the ibmc plug-in is removed. helm plugin list Verify that the IBM Cloud Object Storage pods are removed. kubectl get pods -n <namespace> | grep object-storage Verify that the storage classes are removed. kubectl get storageclasses | grep 'ibmc-s3fs'","title":"Removing the IBM OS Plugin"},{"location":"lab-02/REMOVE_RESOURCES/#remove-the-pvc","text":"kubectl delete pvc my-iks-pvc get pv kubectl get pv delete pv","title":"Remove the PVC"},{"location":"lab-02/REMOVE_RESOURCES/#remove-mongodb","text":"helm uninstall mongodb","title":"Remove MongoDB"},{"location":"lab-02/TROUBLESHOOT/","text":"Troubleshooting \u00b6 \u00b6 Additional steps for IBM Kubernetes Service(IKS) only: \u00b6 If the plugin pods show an \"ErrImagePull\" or \"ImagePullBackOff\" error, verify that the image pull secrets to access IBM Cloud Container Registry exist in the \"kube-system\" namespace of your cluster. $ kubectl get secrets -n kube-system | grep icr-io Example output if the secrets exist: ----o/p---- kube-system-icr-io kube-system-us-icr-io kube-system-uk-icr-io kube-system-de-icr-io kube-system-au-icr-io kube-system-jp-icr-io ----end---- If the secrets do not exist in the \"kube-system\" namespace, check if the secrets are available in the \"default\" namespace of your cluster. $ kubectl get secrets -n default | grep icr-io If the secrets are available in the \"default\" namespace, copy the secrets to the \"kube-system\" namespace of your cluster. If the secrets are not available, continue with the next step. $ kubectl get secret -n default default-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-us-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-uk-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-de-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-au-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-jp-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - If the secrets are not available in the \"default\" namespace, you might have an older cluster and must generate the secrets in the \"default\" namespace. i. Generate the secrets in the \"default\" namespace. ibmcloud ks cluster-pull-secret-apply ii. Verify that the secrets are created in the \"default\" namespace. The creation of the secrets might take a few minutes to complete. kubectl get secrets -n default | grep icr-io iii. Run the commands in step 3 to copy the secrets from the \"default\" namespace to the \"kube-system\" namespace. Verify that the image pull secrets are available in the \"kube-system\" namespace. $ kubectl get secrets -n kube-system | grep icr-io Verify that the state of the plugin pods changes to \"Running\". $ kubectl get pods -n kube-system | grep object Verify that installation was successful ```bash $ kubectl get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 60s $ kubectl get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-ns64d 1/1 Running 0 2m10s 10.47.79.90 10.47.79.90 <none> <none> ibmcloud-object-storage-plugin-79cc466c76-c2kcb 1/1 Running 0 2m10s 172.30.73.78 10.47.79.90 <none> <none> ```","title":"Troubleshooting"},{"location":"lab-02/TROUBLESHOOT/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"lab-02/TROUBLESHOOT/#_1","text":"Additional steps for IBM Kubernetes Service(IKS) only:","title":""},{"location":"lab-02/TROUBLESHOOT/#_2","text":"If the plugin pods show an \"ErrImagePull\" or \"ImagePullBackOff\" error, verify that the image pull secrets to access IBM Cloud Container Registry exist in the \"kube-system\" namespace of your cluster. $ kubectl get secrets -n kube-system | grep icr-io Example output if the secrets exist: ----o/p---- kube-system-icr-io kube-system-us-icr-io kube-system-uk-icr-io kube-system-de-icr-io kube-system-au-icr-io kube-system-jp-icr-io ----end---- If the secrets do not exist in the \"kube-system\" namespace, check if the secrets are available in the \"default\" namespace of your cluster. $ kubectl get secrets -n default | grep icr-io If the secrets are available in the \"default\" namespace, copy the secrets to the \"kube-system\" namespace of your cluster. If the secrets are not available, continue with the next step. $ kubectl get secret -n default default-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-us-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-uk-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-de-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-au-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - $ kubectl get secret -n default default-jp-icr-io -o yaml | sed 's/default/kube-system/g' | kubectl -n kube-system create -f - If the secrets are not available in the \"default\" namespace, you might have an older cluster and must generate the secrets in the \"default\" namespace. i. Generate the secrets in the \"default\" namespace. ibmcloud ks cluster-pull-secret-apply ii. Verify that the secrets are created in the \"default\" namespace. The creation of the secrets might take a few minutes to complete. kubectl get secrets -n default | grep icr-io iii. Run the commands in step 3 to copy the secrets from the \"default\" namespace to the \"kube-system\" namespace. Verify that the image pull secrets are available in the \"kube-system\" namespace. $ kubectl get secrets -n kube-system | grep icr-io Verify that the state of the plugin pods changes to \"Running\". $ kubectl get pods -n kube-system | grep object Verify that installation was successful ```bash $ kubectl get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 60s ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 60s ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 60s $ kubectl get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-ns64d 1/1 Running 0 2m10s 10.47.79.90 10.47.79.90 <none> <none> ibmcloud-object-storage-plugin-79cc466c76-c2kcb 1/1 Running 0 2m10s 172.30.73.78 10.47.79.90 <none> <none> ```","title":""},{"location":"lab-03/","text":"Lab 03 - Create a Custom Builder Image for Source-to-Image (S2I) \u00b6 This project contains a Source-to-Image (S2I) builder image and a S2I runtime image which creates an image running Java web applications on Open Liberty . Source-to-Image (S2I) is an open source toolkit for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image. The Open Liberty builder in this lab can be used in two different environments: Local Docker runtime via 's2i', Deployment to OpenShift'. With interpreted languages like python and javascript, the runtime container is also the build container. For example, with a node.js application the 'npm install' is run to build the application and then 'npm start' is run in the same container in order to start the application. However, with compiled languages like Java, the build and runtime processes can be separated. This will allow for slimmer runtime containers for faster application starts and less bloat in the application image. This lab will focus on the second scenario of using a builder image along with a runtime image. (source: https://github.com/openshift/source-to-image/blob/master/docs/runtime_image.md ) Prerequisites \u00b6 The following prerequisites are needed: A Docker Hub account GitHub Account IBM Cloud Account Have followed these steps to get a cluster You can stop after step 11 Setup \u00b6 For this lab we will need to use a docker-in-docker environment so that we can build our images. For this scenario we will be using the labs client of IBM Skills Network . Follow the instructions here to create your client environment. Clone this repository locally and navigate to the newly cloned directory. git clone https://github.com/IBM/s2i-open-liberty-workshop.git -b conference cd s2i-open-liberty-workshop Then we need to install Source to Image. Run the following command to start the installation script. chmod +x setup.sh ./setup.sh To make things easier, we are going to set some environment variables that we can reuse in later commands. Note : Replace Your Username with your actual docker hub username. If you do not have one, go here to create one. export ROOT_FOLDER = $( pwd ) export DOCKER_USERNAME = <your-docker-username> Your root folder should be set to the root of the cloned repository, e.g. /home/project/s2i-open-liberty-workshop , echo $ROOT_FOLDER /home/project/s2i-open-liberty-workshop Build the builder image \u00b6 In this section we will create the first of our two S2I images. This image will be responsible for taking in our source code and building the application binary with Maven. Navigate to the builder image directory cd ${ ROOT_FOLDER } /builder-image Review the ./Dockerfile cat Dockerfile The image uses a Redhat certified Universal Base Image (UBI) from the public container registry at Redhat, FROM registry.access.redhat.com/ubi8/ubi:8.1 You can customize the builder image further, e.g. change the LABEL for maintainer to your name, LABEL maintainer = \"<your-name>\" Now build the builder image. docker build -t $DOCKER_USERNAME /s2i-open-liberty-builder:0.1.0 . Note : Don't miss the . for the current directory at the end of the docker build command, Log in to your Dockerhub account. After running the below command, you will be asked to enter your docker password. docker login -u $DOCKER_USERNAME Push the builder image out to Docker hub. docker push $DOCKER_USERNAME /s2i-open-liberty-builder:0.1.0 With that done, you can now build your runtime image. Build the runtime image \u00b6 In this section you will create the second of our two S2I images. The runtime image will be responsible for taking the compiled binary from the builder image and serving it with the Open Liberty application server. Navigate to the runtime image directory cd $ROOT_FOLDER /runtime-image Review the ./Dockerfile cat Dockerfile Build the runtime image docker build -t $DOCKER_USERNAME /s2i-open-liberty:0.1.0 . Note : Don't miss the . for the current directory at the end of the docker build command, Push the runtime image to Docker hub. docker push $DOCKER_USERNAME /s2i-open-liberty:0.1.0 Now we are ready to build our application with S2I. Use S2I to build the application container \u00b6 In this section, we will use S2I to build our application container image and then we will run the image locally using Docker. Use the builder image and runtime image to build the application image cd $ROOT_FOLDER /web-app Run a multistage S2I build, to build the application. ~/s2i/s2i build . $DOCKER_USERNAME /s2i-open-liberty-builder:0.1.0 authors --runtime-image $DOCKER_USERNAME /s2i-open-liberty:0.1.0 -a /tmp/src/target -a /tmp/src/server.xml Let's break down the above command: s2i build . - Use s2i build in the current directory to build the Docker image by combining the builder image and sources $DOCKER_USERNAME/s2i-open-liberty-builder:0.1.0 - This is the builder image used to build the application authors - name of our application image --runtime-image $DOCKER_USERNAME/s2i-open-liberty:0.1.0 - Take the output of the builder image and run it in this container. -a /tmp/src/target -a /tmp/src/server.xml - The runtime-artifact flag specifies a file or directory to be copies from builder to runtime image. The runtime-artifact is where the builder output is located. These files will be passed into the runtime image. Run the newly built image to start the application on your local machine in the background, docker run -d --rm -p 9080 :9080 authors Check the container is running successfully, docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7ba756f5f45b authors \"/opt/ol/helpers/run\u2026\" 5 seconds ago Up 4 seconds 0 .0.0.0:9080->9080/tcp, 9443 /tcp optimistic_elbakyan Retrieve the authors using curl, curl -X GET \"http://localhost:9080/api/v1/getauthor\" -H \"accept: application/json\" Deployment to OpenShift \u00b6 In the following steps we will be using two deployment strategies: deploy as a traditional Kubernetes Deployment , and build and deployment using templates, OpenShift BuildConfig , and DeploymentConfig . Now that we have the application running locally and have verified that it works, let's deploy it to an OpenShift environment. Log in with your OpenShift Cluster. In preparation of this lab, you claimed a cluster that was created for you, and which is now available via your IBM Cloud dashboard, Browse to your assigned OpenShift cluster Overview page, From the top right, open your OpenShift web console In the OpenShift Web Console, from the profile dropdown click Copy Login Command . Paste the login command to login, e.g. oc login --token = <login-token> --server = https://<cluster-subdomain>:<service-port> Deploying as a traditional Kubernetes deployment \u00b6 For this method, we will deploy our application by creating a kubernetes deployment along with a service and a route. Tag the image that was created in the previous section. export IMAGE = docker.io/ $DOCKER_USERNAME /authors:latest echo $IMAGE docker tag authors $IMAGE Push the image that we built locally using s2i to the OpenShift image registry. docker push $IMAGE Go back to the root folder, cd $ROOT_FOLDER Review the application.yaml file, cat application.yaml This command will add your newly pushed authors image to the deployment yaml file. sed -i \"s|APPLICATION_IMAGE| $IMAGE |\" application.yaml Apply the application.yaml file using the oc cli to create our Deployment, Service, and Route. oc apply -f application.yaml Now let's visit the deployed application. Run the following to get the route to access the application. oc get routes -l app = authors -o go-template = '{{range .items}}{{.spec.host}}{{end}}' Copy and paste the output of the previous command to set a variable $APP_URL, APP_URL = <get-routes-output> Test the application using curl curl -X GET \"http:// $APP_URL /api/v1/getauthor\" -H \"accept: application/json\" Or use the route to your app and paste it into your web browser and add the following to the end of the route: /openapi/ui So your route should appear like the following but without the (...): authors-route-default...appdomain.cloud/openapi/ui It sometimes takes a minute to fully deploy so if you get a message about the application not being available, try again. Using Templates, BuildConfigs, and DeploymentConfigs \u00b6 For this section, you will explore how to build and deploy our application using OpenShift concepts known as templates, build configs and deployment configs. With build configs, the s2i builds are actually happening on the cluster rather than locally. Create a push secret to push the application image to your DockerHub account. You must modify the following command by replacing \\<Docker Password> with your DockerHub password and \\<Docker Email> with your email address for DockerHub. ```bash kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=$DOCKER_USERNAME --docker-password=<Docker Password> --docker-email=<Docker Email> ``` With the secret created we can now give the *builder* service account access to the secret so that it can push the newly created application image to your account. ```bash kubectl patch sa builder --type='json' -p='[{\"op\":\"add\",\"path\":\"/secrets/-\",\"value\":{\"name\":\"regcred\"}}]' ``` Then, we need to create a builder template that contains our builder image build config. See Using Templates . Review the buildTemplate.yaml file, which contains the BuildConfig object for the builder image, cat buildTemplate.yaml Next, let's create the build config and start the first build: oc process -f buildTemplate.yaml -p DOCKER_USERNAME = $DOCKER_USERNAME | oc apply -f - This command will not only create our build config but also kick off the first build to create our builder image and push it to DockerHub. To view the status of the build you can run: oc describe build open-liberty-builder Or view the Build section of the OpenShift console. Now let's take a look at our runtime image build config: cat runtimeTemplate.yaml Then, create the build config for our runtime image and start the first build: oc process -f runtimeTemplate.yaml -p DOCKER_USERNAME = $DOCKER_USERNAME | oc apply -f - To take a look at the build status, run: oc describe build open-liberty-app Or view the Build section of the OpenShift console. Now with our builds run, we can deploy our application. Previously we used a kubernetes deployment object to do this however this time we will use an OpenShift deployment configuration. Both objects are similar and will accomplish the same goals however with deployment configs you have greater control of your application's deployment behavior. You also have the option to set automated triggers to kick off builds and deploys based on image, configuration, or git repository changes. Due to a limitation in our workshop environment, we will not be exploring triggers and utilizing image streams with the integrated OpenShift registry. Let's check out the deployment config template: cat deploymentConfig.yaml Then, create the deploymentConfig oc apply -f deploymentConfig.yaml oc new-app --template authors-app -p DOCKER_USERNAME = $DOCKER_USERNAME To verify that your app is deployed, run: oc get pods You should see a pod named authors2-1- followed by 5 random characters, in the example below they are x58fk . This is the application pod. oc get pods NAME READY STATUS RESTARTS AGE authors-deployment-69ff497df6-vz75c 1 /1 Running 0 22h authors2-1-deploy 0 /1 Completed 0 63s authors2-1-x58fk 1 /1 Running 0 60s open-liberty-app-1-build 0 /1 Completed 0 179m open-liberty-builder-1-build 0 /1 Completed 0 3h5m You can also view these on the OpenShift dashboard if you navigate to Workloads > Pods and look for your new pod. Make sure you are in your personal project and not in default . The pod name should start with authors2- . Once you have verified that the new pod is running, enter the following command to view the application routes. oc get routes oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD authors-route authors-route-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors-service http None authors2 authors2-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors2 9080 None Copy the route named authors2 and add it to the end of the command below. export API_URL = View the sample below, however, you will have a different route. export API_URL = authors2-default.osgdcw01-0e3e0ef4c9c6d831e8aa6fe01f33bfc4-0000.sjc04.containers.appdomain.cloud Then test your application with the command below: curl -X GET \"http:// $API_URL /api/v1/getauthor\" -H \"accept: application/json\" You should then get a response back from the application that we just deployed: curl -X GET \"http:// $API_URL /api/v1/getauthor\" -H \"accept: application/json\" { \"name\" : \"Oliver Rodriguez\" , \"twitter\" : \"https://twitter.com/heres__ollie\" , \"blog\" : \"https://developer.ibm.com\" Optional: Customizing your application \u00b6 In this optional section we will create our own copy of the code push the changes to OpenShift. First we need to create your own version of the code repo by creating a fork. This will copy the repo into your GitHub account. Navigate to the lab repo at https://github.com/IBM/s2i-open-liberty-workshop and click on the Fork button in the upper right of the page. When the repo is done forking, click on the green Clone or download button and copy your git repo url. To make it easier, create an environment variable to hold your repo url. Copy the following command and replace <repo url> with your actual repo url: export REPOSITORY_URL = <repo url> Then, in your terminal, navigate to the $HOME directory to clone your repo locally and run the following commands: mkdir $HOME /tmp cd $HOME /tmp git clone $REPOSITORY_URL cd s2i-open-liberty-workshop Now that we have our own copy, let's push a change and test it out. From your browser, navigate to the GitHub repo that you forked. Click on the Branch dropdown and select conference . Then navigate to the file at web-app/src/main/java/com/ibm/authors/GetAuthor.java Click on the pencil icon in the upper right of the code to enter editing mode. On lines 56-59 edit the name, twitter, and blog to your own information or fake information if you'd like. Author author = new Author (); author . name = \"Oliver Rodriguez\" ; author . twitter = \"https://twitter.com/heres__ollie\" ; author . blog = \"http://developer.ibm.com\" ; Scroll down and click on Commit changes . With the changes pushed, we can now rebuild and redeploy the application. Follow the following steps: Build the builder image: oc process -f buildTemplate.yaml -p DOCKER_USERNAME = $DOCKER_USERNAME -p SOURCE_REPOSITORY_URL = $REPOSITORY_URL -p APP_NAME = authors-3 | oc apply -f - oc start-build open-liberty-builder Ensure that the previous build is finished, then start the runtime build: oc start-build open-liberty-app Ensure that the runtime image build is finished, then deploy the app with the following command: oc new-app --template authors-app -p DOCKER_USERNAME = $DOCKER_USERNAME -p APP_NAME = authors-3 Once you have verified that the application is deployed and the new pod is running, enter the following command to view the application routes. oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD authors-route authors-route-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors-service http None authors2 authors2-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors2 9080 None Copy the route named authors-3 and add it to the end of the command below. export API_URL = View the sample below, however, you will have a different route. export API_URL = authors2-default.osgdcw01-0e3e0ef4c9c6d831e8aa6fe01f33bfc4-0000.sjc04.containers.appdomain.cloud Then test your application with the command below: curl -X GET \"http:// $API_URL /api/v1/getauthor\" -H \"accept: application/json\" You should then see the info that you edited in the file earlier. Conclusion \u00b6 In this lab we have explored building our own custom s2i images for building containerized application from source code. We utilized a multi stage s2i process that separated the build environment from the runtime environment which allowed for us to have a slimmer application image. Then, we deployed the application as a traditional Kubernetes deployment. Lastly, we explored how to build and deploy the application using templates, OpenShift build configs, and deployment configs. Go back to the Summary .","title":"Lab 3. Source-to-Image (S2I)"},{"location":"lab-03/#lab-03-create-a-custom-builder-image-for-source-to-image-s2i","text":"This project contains a Source-to-Image (S2I) builder image and a S2I runtime image which creates an image running Java web applications on Open Liberty . Source-to-Image (S2I) is an open source toolkit for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image. The Open Liberty builder in this lab can be used in two different environments: Local Docker runtime via 's2i', Deployment to OpenShift'. With interpreted languages like python and javascript, the runtime container is also the build container. For example, with a node.js application the 'npm install' is run to build the application and then 'npm start' is run in the same container in order to start the application. However, with compiled languages like Java, the build and runtime processes can be separated. This will allow for slimmer runtime containers for faster application starts and less bloat in the application image. This lab will focus on the second scenario of using a builder image along with a runtime image. (source: https://github.com/openshift/source-to-image/blob/master/docs/runtime_image.md )","title":"Lab 03 - Create a Custom Builder Image for Source-to-Image (S2I)"},{"location":"lab-03/#prerequisites","text":"The following prerequisites are needed: A Docker Hub account GitHub Account IBM Cloud Account Have followed these steps to get a cluster You can stop after step 11","title":"Prerequisites"},{"location":"lab-03/#setup","text":"For this lab we will need to use a docker-in-docker environment so that we can build our images. For this scenario we will be using the labs client of IBM Skills Network . Follow the instructions here to create your client environment. Clone this repository locally and navigate to the newly cloned directory. git clone https://github.com/IBM/s2i-open-liberty-workshop.git -b conference cd s2i-open-liberty-workshop Then we need to install Source to Image. Run the following command to start the installation script. chmod +x setup.sh ./setup.sh To make things easier, we are going to set some environment variables that we can reuse in later commands. Note : Replace Your Username with your actual docker hub username. If you do not have one, go here to create one. export ROOT_FOLDER = $( pwd ) export DOCKER_USERNAME = <your-docker-username> Your root folder should be set to the root of the cloned repository, e.g. /home/project/s2i-open-liberty-workshop , echo $ROOT_FOLDER /home/project/s2i-open-liberty-workshop","title":"Setup"},{"location":"lab-03/#build-the-builder-image","text":"In this section we will create the first of our two S2I images. This image will be responsible for taking in our source code and building the application binary with Maven. Navigate to the builder image directory cd ${ ROOT_FOLDER } /builder-image Review the ./Dockerfile cat Dockerfile The image uses a Redhat certified Universal Base Image (UBI) from the public container registry at Redhat, FROM registry.access.redhat.com/ubi8/ubi:8.1 You can customize the builder image further, e.g. change the LABEL for maintainer to your name, LABEL maintainer = \"<your-name>\" Now build the builder image. docker build -t $DOCKER_USERNAME /s2i-open-liberty-builder:0.1.0 . Note : Don't miss the . for the current directory at the end of the docker build command, Log in to your Dockerhub account. After running the below command, you will be asked to enter your docker password. docker login -u $DOCKER_USERNAME Push the builder image out to Docker hub. docker push $DOCKER_USERNAME /s2i-open-liberty-builder:0.1.0 With that done, you can now build your runtime image.","title":"Build the builder image"},{"location":"lab-03/#build-the-runtime-image","text":"In this section you will create the second of our two S2I images. The runtime image will be responsible for taking the compiled binary from the builder image and serving it with the Open Liberty application server. Navigate to the runtime image directory cd $ROOT_FOLDER /runtime-image Review the ./Dockerfile cat Dockerfile Build the runtime image docker build -t $DOCKER_USERNAME /s2i-open-liberty:0.1.0 . Note : Don't miss the . for the current directory at the end of the docker build command, Push the runtime image to Docker hub. docker push $DOCKER_USERNAME /s2i-open-liberty:0.1.0 Now we are ready to build our application with S2I.","title":"Build the runtime image"},{"location":"lab-03/#use-s2i-to-build-the-application-container","text":"In this section, we will use S2I to build our application container image and then we will run the image locally using Docker. Use the builder image and runtime image to build the application image cd $ROOT_FOLDER /web-app Run a multistage S2I build, to build the application. ~/s2i/s2i build . $DOCKER_USERNAME /s2i-open-liberty-builder:0.1.0 authors --runtime-image $DOCKER_USERNAME /s2i-open-liberty:0.1.0 -a /tmp/src/target -a /tmp/src/server.xml Let's break down the above command: s2i build . - Use s2i build in the current directory to build the Docker image by combining the builder image and sources $DOCKER_USERNAME/s2i-open-liberty-builder:0.1.0 - This is the builder image used to build the application authors - name of our application image --runtime-image $DOCKER_USERNAME/s2i-open-liberty:0.1.0 - Take the output of the builder image and run it in this container. -a /tmp/src/target -a /tmp/src/server.xml - The runtime-artifact flag specifies a file or directory to be copies from builder to runtime image. The runtime-artifact is where the builder output is located. These files will be passed into the runtime image. Run the newly built image to start the application on your local machine in the background, docker run -d --rm -p 9080 :9080 authors Check the container is running successfully, docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7ba756f5f45b authors \"/opt/ol/helpers/run\u2026\" 5 seconds ago Up 4 seconds 0 .0.0.0:9080->9080/tcp, 9443 /tcp optimistic_elbakyan Retrieve the authors using curl, curl -X GET \"http://localhost:9080/api/v1/getauthor\" -H \"accept: application/json\"","title":"Use S2I to build the application container"},{"location":"lab-03/#deployment-to-openshift","text":"In the following steps we will be using two deployment strategies: deploy as a traditional Kubernetes Deployment , and build and deployment using templates, OpenShift BuildConfig , and DeploymentConfig . Now that we have the application running locally and have verified that it works, let's deploy it to an OpenShift environment. Log in with your OpenShift Cluster. In preparation of this lab, you claimed a cluster that was created for you, and which is now available via your IBM Cloud dashboard, Browse to your assigned OpenShift cluster Overview page, From the top right, open your OpenShift web console In the OpenShift Web Console, from the profile dropdown click Copy Login Command . Paste the login command to login, e.g. oc login --token = <login-token> --server = https://<cluster-subdomain>:<service-port>","title":"Deployment to OpenShift"},{"location":"lab-03/#deploying-as-a-traditional-kubernetes-deployment","text":"For this method, we will deploy our application by creating a kubernetes deployment along with a service and a route. Tag the image that was created in the previous section. export IMAGE = docker.io/ $DOCKER_USERNAME /authors:latest echo $IMAGE docker tag authors $IMAGE Push the image that we built locally using s2i to the OpenShift image registry. docker push $IMAGE Go back to the root folder, cd $ROOT_FOLDER Review the application.yaml file, cat application.yaml This command will add your newly pushed authors image to the deployment yaml file. sed -i \"s|APPLICATION_IMAGE| $IMAGE |\" application.yaml Apply the application.yaml file using the oc cli to create our Deployment, Service, and Route. oc apply -f application.yaml Now let's visit the deployed application. Run the following to get the route to access the application. oc get routes -l app = authors -o go-template = '{{range .items}}{{.spec.host}}{{end}}' Copy and paste the output of the previous command to set a variable $APP_URL, APP_URL = <get-routes-output> Test the application using curl curl -X GET \"http:// $APP_URL /api/v1/getauthor\" -H \"accept: application/json\" Or use the route to your app and paste it into your web browser and add the following to the end of the route: /openapi/ui So your route should appear like the following but without the (...): authors-route-default...appdomain.cloud/openapi/ui It sometimes takes a minute to fully deploy so if you get a message about the application not being available, try again.","title":"Deploying as a traditional Kubernetes deployment"},{"location":"lab-03/#using-templates-buildconfigs-and-deploymentconfigs","text":"For this section, you will explore how to build and deploy our application using OpenShift concepts known as templates, build configs and deployment configs. With build configs, the s2i builds are actually happening on the cluster rather than locally. Create a push secret to push the application image to your DockerHub account. You must modify the following command by replacing \\<Docker Password> with your DockerHub password and \\<Docker Email> with your email address for DockerHub. ```bash kubectl create secret docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=$DOCKER_USERNAME --docker-password=<Docker Password> --docker-email=<Docker Email> ``` With the secret created we can now give the *builder* service account access to the secret so that it can push the newly created application image to your account. ```bash kubectl patch sa builder --type='json' -p='[{\"op\":\"add\",\"path\":\"/secrets/-\",\"value\":{\"name\":\"regcred\"}}]' ``` Then, we need to create a builder template that contains our builder image build config. See Using Templates . Review the buildTemplate.yaml file, which contains the BuildConfig object for the builder image, cat buildTemplate.yaml Next, let's create the build config and start the first build: oc process -f buildTemplate.yaml -p DOCKER_USERNAME = $DOCKER_USERNAME | oc apply -f - This command will not only create our build config but also kick off the first build to create our builder image and push it to DockerHub. To view the status of the build you can run: oc describe build open-liberty-builder Or view the Build section of the OpenShift console. Now let's take a look at our runtime image build config: cat runtimeTemplate.yaml Then, create the build config for our runtime image and start the first build: oc process -f runtimeTemplate.yaml -p DOCKER_USERNAME = $DOCKER_USERNAME | oc apply -f - To take a look at the build status, run: oc describe build open-liberty-app Or view the Build section of the OpenShift console. Now with our builds run, we can deploy our application. Previously we used a kubernetes deployment object to do this however this time we will use an OpenShift deployment configuration. Both objects are similar and will accomplish the same goals however with deployment configs you have greater control of your application's deployment behavior. You also have the option to set automated triggers to kick off builds and deploys based on image, configuration, or git repository changes. Due to a limitation in our workshop environment, we will not be exploring triggers and utilizing image streams with the integrated OpenShift registry. Let's check out the deployment config template: cat deploymentConfig.yaml Then, create the deploymentConfig oc apply -f deploymentConfig.yaml oc new-app --template authors-app -p DOCKER_USERNAME = $DOCKER_USERNAME To verify that your app is deployed, run: oc get pods You should see a pod named authors2-1- followed by 5 random characters, in the example below they are x58fk . This is the application pod. oc get pods NAME READY STATUS RESTARTS AGE authors-deployment-69ff497df6-vz75c 1 /1 Running 0 22h authors2-1-deploy 0 /1 Completed 0 63s authors2-1-x58fk 1 /1 Running 0 60s open-liberty-app-1-build 0 /1 Completed 0 179m open-liberty-builder-1-build 0 /1 Completed 0 3h5m You can also view these on the OpenShift dashboard if you navigate to Workloads > Pods and look for your new pod. Make sure you are in your personal project and not in default . The pod name should start with authors2- . Once you have verified that the new pod is running, enter the following command to view the application routes. oc get routes oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD authors-route authors-route-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors-service http None authors2 authors2-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors2 9080 None Copy the route named authors2 and add it to the end of the command below. export API_URL = View the sample below, however, you will have a different route. export API_URL = authors2-default.osgdcw01-0e3e0ef4c9c6d831e8aa6fe01f33bfc4-0000.sjc04.containers.appdomain.cloud Then test your application with the command below: curl -X GET \"http:// $API_URL /api/v1/getauthor\" -H \"accept: application/json\" You should then get a response back from the application that we just deployed: curl -X GET \"http:// $API_URL /api/v1/getauthor\" -H \"accept: application/json\" { \"name\" : \"Oliver Rodriguez\" , \"twitter\" : \"https://twitter.com/heres__ollie\" , \"blog\" : \"https://developer.ibm.com\"","title":"Using Templates, BuildConfigs, and DeploymentConfigs"},{"location":"lab-03/#optional-customizing-your-application","text":"In this optional section we will create our own copy of the code push the changes to OpenShift. First we need to create your own version of the code repo by creating a fork. This will copy the repo into your GitHub account. Navigate to the lab repo at https://github.com/IBM/s2i-open-liberty-workshop and click on the Fork button in the upper right of the page. When the repo is done forking, click on the green Clone or download button and copy your git repo url. To make it easier, create an environment variable to hold your repo url. Copy the following command and replace <repo url> with your actual repo url: export REPOSITORY_URL = <repo url> Then, in your terminal, navigate to the $HOME directory to clone your repo locally and run the following commands: mkdir $HOME /tmp cd $HOME /tmp git clone $REPOSITORY_URL cd s2i-open-liberty-workshop Now that we have our own copy, let's push a change and test it out. From your browser, navigate to the GitHub repo that you forked. Click on the Branch dropdown and select conference . Then navigate to the file at web-app/src/main/java/com/ibm/authors/GetAuthor.java Click on the pencil icon in the upper right of the code to enter editing mode. On lines 56-59 edit the name, twitter, and blog to your own information or fake information if you'd like. Author author = new Author (); author . name = \"Oliver Rodriguez\" ; author . twitter = \"https://twitter.com/heres__ollie\" ; author . blog = \"http://developer.ibm.com\" ; Scroll down and click on Commit changes . With the changes pushed, we can now rebuild and redeploy the application. Follow the following steps: Build the builder image: oc process -f buildTemplate.yaml -p DOCKER_USERNAME = $DOCKER_USERNAME -p SOURCE_REPOSITORY_URL = $REPOSITORY_URL -p APP_NAME = authors-3 | oc apply -f - oc start-build open-liberty-builder Ensure that the previous build is finished, then start the runtime build: oc start-build open-liberty-app Ensure that the runtime image build is finished, then deploy the app with the following command: oc new-app --template authors-app -p DOCKER_USERNAME = $DOCKER_USERNAME -p APP_NAME = authors-3 Once you have verified that the application is deployed and the new pod is running, enter the following command to view the application routes. oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD authors-route authors-route-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors-service http None authors2 authors2-default.your-roks-43-1n-cl-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud authors2 9080 None Copy the route named authors-3 and add it to the end of the command below. export API_URL = View the sample below, however, you will have a different route. export API_URL = authors2-default.osgdcw01-0e3e0ef4c9c6d831e8aa6fe01f33bfc4-0000.sjc04.containers.appdomain.cloud Then test your application with the command below: curl -X GET \"http:// $API_URL /api/v1/getauthor\" -H \"accept: application/json\" You should then see the info that you edited in the file earlier.","title":"Optional: Customizing your application"},{"location":"lab-03/#conclusion","text":"In this lab we have explored building our own custom s2i images for building containerized application from source code. We utilized a multi stage s2i process that separated the build environment from the runtime environment which allowed for us to have a slimmer application image. Then, we deployed the application as a traditional Kubernetes deployment. Lastly, we explored how to build and deploy the application using templates, OpenShift build configs, and deployment configs. Go back to the Summary .","title":"Conclusion"},{"location":"lab-03/skillsNetwork/","text":"IBM Skills Network Access \u00b6 Create an account \u00b6 Navigate to https://labs.cognitiveclass.ai/register , Create a new account with a Social login (LinkedIn, Google, Github or Facebook), or click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger. With that done, you can return back to the lab. <- Back to the lab","title":"IBM Skills Network Access"},{"location":"lab-03/skillsNetwork/#ibm-skills-network-access","text":"","title":"IBM Skills Network Access"},{"location":"lab-03/skillsNetwork/#create-an-account","text":"Navigate to https://labs.cognitiveclass.ai/register , Create a new account with a Social login (LinkedIn, Google, Github or Facebook), or click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger. With that done, you can return back to the lab. <- Back to the lab","title":"Create an account"},{"location":"resources/","text":"Additional resources \u00b6 Resources \u00b6 Cloud Native Computing Foundation Cloud Native Security Conference IBM Developer Survey \u00b6 Tell us how we did","title":"Additional resources"},{"location":"resources/#additional-resources","text":"","title":"Additional resources"},{"location":"resources/#resources","text":"Cloud Native Computing Foundation Cloud Native Security Conference IBM Developer","title":"Resources"},{"location":"resources/#survey","text":"Tell us how we did","title":"Survey"}]}